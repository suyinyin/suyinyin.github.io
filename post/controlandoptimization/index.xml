<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Some interesting control and optimization algorithm. | Yinyin</title>
    <link>/post/controlandoptimization/</link>
      <atom:link href="/post/controlandoptimization/index.xml" rel="self" type="application/rss+xml" />
    <description>Some interesting control and optimization algorithm.</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright © Yinyin SU. All rights reserved.</copyright><lastBuildDate>Thu, 03 Sep 2020 20:30:09 +0800</lastBuildDate>
    <image>
      <url>/post/controlandoptimization/featured.png</url>
      <title>Some interesting control and optimization algorithm.</title>
      <link>/post/controlandoptimization/</link>
    </image>
    
    <item>
      <title>Preceptron algorithm</title>
      <link>/post/controlandoptimization/preceptron/</link>
      <pubDate>Thu, 03 Sep 2020 20:30:09 +0800</pubDate>
      <guid>/post/controlandoptimization/preceptron/</guid>
      <description>&lt;p&gt;In machine learning, the &lt;strong&gt;perceptron&lt;/strong&gt; is an algorithm for supervised learning of binary classfiers. The 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Perceptron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;perceptron algorithm&lt;/a&gt; was invented 1958 at the Cornell Aeronautical lab by Frank Rosenblatt.&lt;/p&gt;
&lt;h3 id=&#34;perceptron-model&#34;&gt;Perceptron model&lt;/h3&gt;
&lt;p&gt;For input space (featured space) $\mathcal{X} = {{\mathbf{R}}^{n}}$ and  output space $\mathcal{Y} =\{-1,  +1\}$, the perceptron model can be built by:
$$f(x) = sign(\omega \centerdot x+b)$$
$$sign(x) =
\begin{cases}
+1 &amp;amp; \text{if } x \ge 0,\\&lt;br&gt;
-1 &amp;amp; \text{if } x &amp;lt; 0.
\end{cases}$$
where $\omega$ and $b$ are the weight and bias for the above function respectively. Through being trained by the give training data $T = \{ (x_0, y_0), (x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}$, where $x_i \in \mathcal{X} =  {{\mathbf{R}}^{n}}$ and $y_i \in \mathcal{Y}=\{-1, +1\}$, $i = 1, 2, \cdots, N$. The parameters $\omega$ and $b$ can be leanned through the mentioned data set.  The learned linear hyperplane can divide the training data into two parts labelling +1 and -1.  The schematic diagram is shown as follows.&lt;/p&gt;






  



  
  











&lt;figure id=&#34;figure-the-schematic-diagram-of-perceptron&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/controlandoptimization/preceptron/preceptron_hu425ef45277cde0d0b8f5a13aa87ba3d6_18766_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;The schematic diagram of perceptron&#34;&gt;


  &lt;img data-src=&#34;/post/controlandoptimization/preceptron/preceptron_hu425ef45277cde0d0b8f5a13aa87ba3d6_18766_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;486&#34; height=&#34;419&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    The schematic diagram of perceptron
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;perceptron-strategy&#34;&gt;Perceptron strategy&lt;/h3&gt;
&lt;p&gt;In perceptron algorithm, the total distance to the hyperplane is defined to depict its cost function:
$$L\left( \omega ,b \right)=-\sum\limits_{{{x}_{i}}\in M}{{{y}_{i}}\left( \omega \cdot {{x}_{i}}+b \right)}$$
where $M$ is misclassfication data set.&lt;/p&gt;
&lt;h3 id=&#34;perceptron-learning-algorithm&#34;&gt;Perceptron learning algorithm&lt;/h3&gt;
&lt;p&gt;For the given training data set, the parameters $\omega$ and $b$ can be optimized by minimizing the cost function:
$$\underset{\omega ,b}{\mathop{\min }},L\left( \omega ,b \right)=-\sum\limits_{{{x}_{i}}\in M}{{{y}_{i}}\left( \omega \cdot {{x}_{i}}+b \right)}$$
We can use stochastic gradient descent method to solve the above optimized problem. Take partial derivative with respect to $\omega$ and $b$, the iteration strategy can be obtained:
$$\begin{align}
&amp;amp; \omega \leftarrow \omega +\eta {{y}_{i}}{{x}_{i}} \&lt;br&gt;
&amp;amp; b\leftarrow b+\eta {{y}_{i}} \&lt;br&gt;
\end{align}$$
where $\eta$ is a learning rate hyperparameter set by user.&lt;/p&gt;
&lt;p&gt;Based on the aforementioned discussion, the perceptron algorithm can be derived:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Preceptron algorithm&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; training data set: $T = { (x_0, y_0), (x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)}$, $i = 1, 2, 3, \cdots, N$; learning rate $\eta (0 &amp;lt; \eta \le 1)$;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; $\omega$, $b$ and preceptron model: $f(x) = sign(\omega \centerdot x+b)$&lt;/li&gt;
&lt;li&gt;Initialize $\omega_0, b_0$;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;while&lt;/strong&gt;: Choosing the single instance $(x_i, y_i)$ from $T$ until no misclassfication data contained in the set;&lt;/li&gt;
&lt;li&gt;  &lt;strong&gt;if&lt;/strong&gt;  $y_i \left( \omega \cdot x_i +b\right) \le 0$&lt;/li&gt;
&lt;li&gt;   $\omega \leftarrow \omega +\eta {y_i}{x_i}$ and $b\leftarrow b+\eta {{y}_{i}}$&lt;/li&gt;
&lt;li&gt;  &lt;strong&gt;endif&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;end while&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;implementing-the-algorithm-in-python3&#34;&gt;Implementing the algorithm in python3&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from matplotlib import pyplot as plt
import numpy as np
from matplotlib.animation import FuncAnimation

# define a function to realize preceptron algorithm.
def PrecessingPreceptron(point, learning_rate):
    w_set = []
    b_set = []
    w = np.array([0,0])
    b = 0
    flag = 1
    while flag:
        flag = 0
        i = 0
        for i in range(len(point)):
            if point[i,2]*(np.matmul(w, point[i,0:2]) + b) &amp;lt;= 0:
                w = w + learning_rate * point[i,0:2] * point[i,2]
                b = b + learning_rate * point[i,2]
                w_set.append(w)
                b_set.append(b)
                flag = 1
                break
        if flag == 0:
            break
    return w_set, b_set

# DrawFigure is used to draw the process
def DrawFigure(count, w_set, b_set, gap):
    point_x_1 = 0
    point_x_2 = 5
    if count == 0:
        line.set_data([], [])
        return line,
    if gap*count &amp;lt; len(w_set):
        w = w_set[count*gap]
        b = b_set[count*gap]
    else:
        w = w_set[-1]
        b = b_set[-1]
    if w[1] != 0:
        point_y_1 = -(w[0] * point_x_1 + b) / w[1]
        point_y_2 = -(w[0] * point_x_2 + b) / w[1]
        thisx = np.array([point_x_1, point_x_2])
        thisy = np.array([point_y_1, point_y_2])
        line.set_data(thisx, thisy)
        return line,

if __name__ == &amp;quot;__main__&amp;quot;:
    # define input data set
    traning_data = np.array([[3, 4, 1],[4, 3, 1], [3,3,-1],[1,2,-1],[3,5,1],[4,7,1],[3,2,-1],[1.5,5,-1]])
    learning_rate = 0.7
    fig, ax = plt.subplots()
    line, = ax.plot([], [], &#39;k-&#39;)
    for index in range(len(traning_data)):
        if traning_data[index,2] == 1:
            ax.plot(traning_data[index, 0], traning_data[index, 1], &#39;ro&#39;)
        else:
            ax.plot(traning_data[index,0], traning_data[index,1],&#39;bo&#39;)
    plt.xlabel(&#39;$x_1$&#39;)
    plt.ylabel(&#39;$x_2$&#39;)
    plt.title(&#39;Preceptron algorithm learning precess&#39;)
    w_set, b_set = PrecessingPreceptron(traning_data, learning_rate)
    anim = FuncAnimation(fig, DrawFigure,frames=np.arange(0, 15), fargs=(w_set, b_set, 50), interval=300, blit=True)
    anim.save(&#39;preceptron.gif&#39;, writer=&#39;imagemagick&#39;)
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;
&lt;img src=&#34;preceptron.gif&#34; alt=&#34;Alt Text&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;notes&#34;&gt;Notes&lt;/h3&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;It can be testified that the perceptron algorithm could converge at last within the limited steps of iterations.&lt;/li&gt;
&lt;li&gt;The original algorithm have multiple solutions coming from: 1. The initial parameters $\omega_0, b_0$; 2. the selection order of misclassficated point in the learning process.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Alogorithm</title>
      <link>/post/controlandoptimization/alogorithm/</link>
      <pubDate>Tue, 11 Aug 2020 16:18:34 +0800</pubDate>
      <guid>/post/controlandoptimization/alogorithm/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
