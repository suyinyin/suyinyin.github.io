[{"authors":["admin"],"categories":null,"content":"I am a PhD Student of control and robotics at the Automatic Control Lab supervised by Professor James Lam and at the Biorobotic and Control Lab supervised by Professor Zheng Wang respectively. My research interests include soft robotics, applying control and optimization algorithms in robot, robotic system and kinematic and dynamic of robot.\nI like playing basketball and football. Lebron James and Antoine Griezmann are my favorite basketball and football player, respectively. Besides, I am interested in fishing and grasp some skills in ice skating and skiing. If you have the same hobbies or interests in my research field, please do not hesitate to contact me.\n","date":1603369809,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1603369809,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/yinyin-su/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yinyin-su/","section":"authors","summary":"I am a PhD Student of control and robotics at the Automatic Control Lab supervised by Professor James Lam and at the Biorobotic and Control Lab supervised by Professor Zheng Wang respectively.","tags":null,"title":"Yinyin SU","type":"authors"},{"authors":["Yinyin SU"],"categories":[],"content":"Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) are all efficient ways to transform the data points in high dimesion to the corresponding feature points in low dimension without losing the principal feature. Futhermore, the low-dimensional data can be visualized in frame, can feel by human. Hence, In this post, I simply summarized some mathematics model for PCA and t-SNE algorithms, implemented these methods repectively, and presented some advangtages and disadvantages of these alogorithms. At the end, some important hyperparameters that effect the performance of t-SNE are described to remind readers to choose the proper parameters. Some other demension reduction methods were summarized as follows:\n  PCA (linear); t-SNE (non-parametric/ nonlinear); Sammon mapping (nonlinear); Isomap (nonlinear); LLE (nonlinear); CCA (nonlinear); SNE (nonlinear); MVU (nonlinear); Laplacian Eigenmaps (nonlinear),   and their attributes were shown in the below figure. 1. PCA Principal Component Analysis (PCA) is a process of computing principal components using the first few principal component and ignoring the rest. The goals of PCA are to\n  extract the most important information from the data table; compress the size of the data set by keeping only this important information; simplify the description of the data set; and analyze the structure of the observations and the variables.   The PCA can be defined commonly by two methods, resulting in the same algorithme. As shown in the following figure, principal component analysis seeks a space of lower dimensionality, known as the principal subspace and denoted by the magenta line, such that the orthogonal projection of the data points (red dots) onto this subspace maximizes the variance of the projected points (green dots). An alternative definition of PCA is based on minimizing the sum-of-squares of the projection errors, indicated by the blue lines. 1.1 Maxiunum variance formulation Consider a data set of observations $\\left\\{\\mathbf{x}_{n}\\right\\}$ where $n = 1, \\cdots, N$ and the dimensionality of $\\mathbf{x}_{n}$ is $D$. The PCA can project the data on to lower dimensional space with dimensionality $M \u0026lt; D$ while maximizing the variance of the projected data. We define unit vector ${{w}_{1}}$, which is column vector, as the first principal direction and each data point $\\mathbf{x}_{n}$ is then projected onto a scalar value ${{z}_{1}^n} = \\mathbf{{w}_{1}^{T}}{{{x}}_{n}}$. Based on the above definition, we preject all data points onto $\\mathbf{w_1}$ direction, maximizing the variance along $\\mathbf{w_1}$ direction, $$\\max Var\\left( {{\\mathbf{z}}_{1}} \\right)=\\frac{1}{N}\\sum\\limits_{1}^{N}{{{\\left( {{\\mathbf{z}}_{1}}-{{{\\mathbf{\\bar{z}}}}_{1}} \\right)}^{2}}},{{\\left|| {{\\mathbf{w}}_{1}} \\right||}_{2}}=1, $$ where $\\mathbf{z}_{1}$ is $[z_1^{1}, z_2^{1}, \\cdots, z_N^{1}]$, and ${{{\\mathbf{\\bar{z}}}}_{1}}$ is sample set mean, given by ${{\\mathbf{\\bar{z}}}_{1}}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}{z_{i}^{1}}$. The above formulation is also used in other principal direction like $\\mathbf{w_2}$, so the variance of data set $\\mathbf{z_2}$ can be derived by $$\\max Var\\left( {{\\mathbf{z}}_{2}} \\right)=\\frac{1}{N}\\sum\\limits_{1}^{N}{{{\\left( {{\\mathbf{z}}_{2}}-{{{\\mathbf{\\bar{z}}}}_{2}} \\right)}^{2}}},{{\\left|| {{\\mathbf{w}}_{2}} \\right||}_{2}}=1$$ As a result, the low-dimensional data points $$\\mathbf{z}=W\\mathbf{x}, $$\nwhere $W$ is $\\left[ \\begin{matrix} {{\\mathbf{w}}_{1}} \u0026amp; {{\\mathbf{w}}_{2}} \u0026amp; {{\\mathbf{w}}_{3}} \u0026amp; \\cdots \\end{matrix} \\right]$, and it is a orthogonal matrix. The variance of data set $\\mathbf{z_1}$ can be derived\n\\begin{align} Var\\left( {{{\\mathbf{{z}}}}_{1}} \\right) \u0026amp; =\\frac{1}{N}{{\\sum\\limits_{i=1}^{N}{\\left( z_{i}^{1}-{{{\\bar{z}}}^{1}} \\right)}}^{2}} \\nonumber \\\\\n\u0026amp; =\\frac{1}{N}{{\\sum\\limits_{i=1}^{N}{\\left( \\mathbf{w}_{1}^{\\text{T}}x_{i}^{1}-\\mathbf{w}_{1}^{\\text{T}}{{{\\bar{x}}}^{1}} \\right)}}^{2}} \\nonumber\\\\\n\u0026amp; =\\frac{1}{N}{{\\sum\\limits_{i=1}^{N}{\\left[ \\mathbf{w}_{1}^{\\text{T}}\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right) \\right]}}^{2}}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}{\\mathbf{w}_{1}^{\\text{T}}\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right)\\mathbf{w}_{1}^{\\text{T}}\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right)} \\nonumber\\\\\n\u0026amp; =\\frac{1}{N}\\sum\\limits_{i=1}^{N}{\\mathbf{w}_{1}^{\\text{T}}\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right)}{{\\left[ \\mathbf{w}_{1}^{\\text{T}}\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right) \\right]}^{\\text{T}}} \\nonumber\\\\\n\u0026amp; =\\mathbf{w}_{1}^{\\text{T}}\\left[ \\frac{1}{N}\\sum\\limits_{i=1}^{N}{\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right)}{{\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right)}^{\\text{T}}} \\right]{{\\mathbf{w}}_{1}} \\nonumber\\\\\n\u0026amp; = \\mathbf{w}_{1}^{\\text{T}} cov\\left( \\mathbf x\\right) \\mathbf{w}_{1}\\nonumber\\\\\n\\end{align}\nwhere $S = cov\\left( \\mathbf x\\right)$ is covariance matrix of data set $\\left\\{\\mathbf{x}_{n}\\right\\}$, and it is a sysmetric and semidefinite matrix. Eventually, if we want to obtain the principal vector $\\mathbf{w_1}$, the optimization equation is\n\\begin{align} \\underset{{{\\mathbf{w}}_{1}}}{\\mathop{\\max }},\\mathbf{w}_{1}^{\\text{T}}S{{\\mathbf{w}}_{1}},\\\\\ns.t.\\quad \\mathbf{w}_{1}^{\\text{T}}{{\\mathbf{w}}_{1}} = 1. \\end{align}\nUsing the Langrange multiplier, $$g\\left( {{\\mathbf{w}}_{1}} \\right)=\\mathbf{w}_{1}^{\\text{T}}S{{\\mathbf{w}}_{1}}+\\alpha \\left( 1-\\mathbf{w}_{1}^{\\text{T}}{{\\mathbf{w}}_{1}} \\right), $$\nlet $\\frac{\\partial g\\left( {{\\mathbf{w}}_{1}} \\right)}{\\partial {{\\mathbf{w}}_{1}}}=0$, the following equation can be obtianed $$\\frac{\\partial g\\left( {{\\mathbf{w}}_{1}} \\right)}{\\partial {{\\mathbf{w}}_{1}}}=S{{\\mathbf{w}}_{1}}-\\alpha{{\\mathbf{w}}_{1}} =0$$ Hence, the ${{\\mathbf{w}}_{1}}$ and $\\alpha$ are the eigenvector and the corresponding eigenvalue of covariance matrix $S$. Moreover, $\\alpha$ is the first largest eigenvalue. Also, the ${{\\mathbf{w}}_{2}}$ is the corresponding eigenvetor related to the second largest eigenvalue of $S$.\n2. t-SNE t-SNE is a mechine learning algorithm for visualazation developed by Sam Roweis and Geoggrey Hinton, well suitied for embedding high-dimensiontal data for visualization in a low-dimensional space of two or three dimension observed directly by human. It is extensively applied in image processing, NLP, genomic data and speech processing. The t-SNE puts emphasis on (1) modeling dissimilar datapoints by means of large pairwise distances, and (2) modeling similar datapoints by means of small pairwise distances. To keep things simple, hereâ€™s a brief overview of working of t-SNE:\n  The algorithms starts by calculating the probability of similarity of points in high-dimensional space and calculating the probability of similarity of points in the corresponding low-dimensional space. The similarity of points is calculated as the conditional probability that a point A would choose point B as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian (normal distribution) centered at A. It then tries to minimize the difference between these conditional probabilities (or similarities) in higher-dimensional and lower-dimensional space for a perfect representation of data points in lower-dimensional space. To measure the minimization of the sum of difference of conditional probability t-SNE minimizes the sum of Kullback-Leibler divergence of overall data points using a gradient descent method.   2.1 Mathematics model Step 1: Define similarity in high-dimensional space t-SNE uses Stochastic Neighbour Embedding (SNE) method to convert the high-dimensional Euclidean distances between data points into conditional probablities that represent their similiarities. The conditional probability ${{p}_{j|i}}$ is defined to represent the similarity, $x_i$ would choose $x_j$ as its neighhour if neighours are chosen in proportion to their probablity density under a Guassian with center at $x_i$. The nearer the data points are, the higher value of ${{p}_{j|i}}$ is. Mathematically, ${{p}_{j|i}}$ can be defined by $${\\displaystyle p_{j\\mid i}={\\frac {\\exp(-\\lVert \\mathbf {x} _{i}-\\mathbf {x} _{j}\\rVert ^{2}/2\\sigma _{i}^{2})}{\\sum _{k\\neq i}\\exp(-\\lVert \\mathbf {x} _{i}-\\mathbf {x} _{k}\\rVert ^{2}/2\\sigma _{i}^{2})}}}, $$ where $\\sigma_i$ is the variance of the Guassian distribution with the mean being $x_i$. Morever $p_{ij} $is defined by $${\\displaystyle p_{ij}={\\frac {p_{j\\mid i}+p_{i\\mid j}}{2N}}}$$ where $p_{ii} = 0$ and $p_{ij} = p_{ji}$.\nStep 2: Define similarity in low-dimensional space For the low-dimensional counterparts $y_i$ and $y_j$ of the high-dimensional data points $x_i$ and $x_j$. It is possible to define a similar conditional probability, donoted by $q_{ij}$ $${\\displaystyle q_{ij}={\\frac {(1+\\lVert \\mathbf {y} _{i}-\\mathbf {y} _{j}\\rVert ^{2})^{-1}}{\\sum _{k}\\sum _{l\\neq k}(1+\\lVert \\mathbf {y} _{k}-\\mathbf {y} _{l}\\rVert ^{2})^{-1}}}}, $$ where $q_{ii} = 0$.\nStep 3: Define cost function to compute $q_{ij}$ If the map points $y_i$ and $y_j$ correctly model the similarity between the high-dimensional datapoints $x_i$ and $x_j$, the conditional probabilities $p_{j|i}$ and $q_{j|i}$ will be equal. Motivated by this observation, SNE aims to find a low-dimensional data representation that minimizes the mismatch between $p_{j|i}$ and $q_{j|i}$. A natural measure of the faithfulness with which $q_{j|i}$ models $p_{j|i}$ is the Kullback-Leibler divergence (which is in this case equal to the cross-entropy up to an additive constant). SNE minimizes the sum of Kullback-Leibler divergences over all datapoints using a gradient descent method. The cost function C is given by $$C= \\sum_{}{\\displaystyle \\mathrm {KL} \\left(P\\parallel Q\\right)=\\sum _{i\\neq j}p_{ij}\\log {\\frac {p_{ij}}{q_{ij}}}}$$ in which $P$ represents the conditional probability distribution over all other datapoints given datapoint $x$, and $Q$ represents the conditional probability distribution over all other map points given map point $y$. The defination of $p_{ij}$ and $q_{ij}$ solves the crowding problem for SNE from Laurens van der Maaten in Visualizing Data using t-SNE. The gradient of the Kullback-Leibler divergence $C$ is derived (the detailed derived procedure was presented in Visualizing Data using t-SNE) $$\\frac{\\delta C}{\\delta {{y}_{i}}}=4\\sum\\limits_{j}{\\left( {{p}_{ij}}-{{q}_{ij}} \\right)}\\left( {{y}_{i}}-{{y}_{j}} \\right){{\\left( 1+{{\\left| {{y}_{i}}-{{y}_{j}} \\right|}^{2}} \\right)}^{-1}}$$\n2.2 Implementing PCA in python3 ## Inherited from personal page of Laurens van der Maaten ## https://lvdmaaten.github.io/tsne/ import numpy as np import matplotlib.pyplot as plt def Hbeta(D=np.array([]), beta=1.0): \u0026quot;\u0026quot;\u0026quot; Compute the perplexity and the P-row for a specific value of the precision of a Gaussian distribution. \u0026quot;\u0026quot;\u0026quot; # Compute P-row and corresponding perplexity P = np.exp(-D.copy() * beta) sumP = sum(P) H = np.log(sumP) + beta * np.sum(D * P) / sumP P = P / sumP return H, P def x2p(X=np.array([]), tol=1e-5, perplexity=30.0): \u0026quot;\u0026quot;\u0026quot; Performs a binary search to get P-values in such a way that each conditional Gaussian has the same perplexity. \u0026quot;\u0026quot;\u0026quot; # Initialize some variables print(\u0026quot;Computing pairwise distances...\u0026quot;) (n, d) = X.shape sum_X = np.sum(np.square(X), 1) D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X) P = np.zeros((n, n)) beta = np.ones((n, 1)) logU = np.log(perplexity) # Loop over all datapoints for i in range(n): # Print progress if i % 500 == 0: print(\u0026quot;Computing P-values for point %d of %d...\u0026quot; % (i, n)) # Compute the Gaussian kernel and entropy for the current precision betamin = -np.inf betamax = np.inf Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] (H, thisP) = Hbeta(Di, beta[i]) # Evaluate whether the perplexity is within tolerance Hdiff = H - logU tries = 0 while np.abs(Hdiff) \u0026gt; tol and tries \u0026lt; 50: # If not, increase or decrease precision if Hdiff \u0026gt; 0: betamin = beta[i].copy() if betamax == np.inf or betamax == -np.inf: beta[i] = beta[i] * 2. else: beta[i] = (beta[i] + betamax) / 2. else: betamax = beta[i].copy() if betamin == np.inf or betamin == -np.inf: beta[i] = beta[i] / 2. else: beta[i] = (beta[i] + betamin) / 2. # Recompute the values (H, thisP) = Hbeta(Di, beta[i]) Hdiff = H - logU tries += 1 # Set the final row of P P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP # Return final P-matrix print(\u0026quot;Mean value of sigma: %f\u0026quot; % np.mean(np.sqrt(1 / beta))) return P def pca(X=np.array([]), no_dims=50): \u0026quot;\u0026quot;\u0026quot; Runs PCA on the NxD array X in order to reduce its dimensionality to no_dims dimensions. \u0026quot;\u0026quot;\u0026quot; print(\u0026quot;Preprocessing the data using PCA...\u0026quot;) (n, d) = X.shape X = X - np.tile(np.mean(X, 0), (n, 1)) (l, M) = np.linalg.eig(np.dot(X.T, X)) Y = np.dot(X, M[:, 0:no_dims]) return Y def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0): \u0026quot;\u0026quot;\u0026quot; Runs t-SNE on the dataset in the NxD array X to reduce its dimensionality to no_dims dimensions. The syntaxis of the function is `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array. Notation of the input parameters: no_dims: Show the dimensional datapoints in 2-dimension space. initial_dims: The PCA is introduced to pre-reduce the initial dimension. After the process, Dimension become 50 from 784. perplexity: It is, in a sense, a guess about the number of close neighbors each point has. Its recommended value (5-50). \u0026quot;\u0026quot;\u0026quot; # Check inputs if isinstance(no_dims, float): print(\u0026quot;Error: array X should have type float.\u0026quot;) return -1 if round(no_dims) != no_dims: print(\u0026quot;Error: number of dimensions should be an integer.\u0026quot;) return -1 # Initialize variables X = pca(X, initial_dims).real (n, d) = X.shape max_iter = 1000 initial_momentum = 0.5 final_momentum = 0.8 eta = 500 min_gain = 0.01 Y = np.random.randn(n, no_dims) dY = np.zeros((n, no_dims)) iY = np.zeros((n, no_dims)) gains = np.ones((n, no_dims)) # Compute P-values P = x2p(X, 1e-5, perplexity) P = P + np.transpose(P) P = P / np.sum(P) P = P * 4.\t# early exaggeration P = np.maximum(P, 1e-12) # Run iterations for iter in range(max_iter): # Compute pairwise affinities sum_Y = np.sum(np.square(Y), 1) num = -2. * np.dot(Y, Y.T) num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y)) num[range(n), range(n)] = 0. Q = num / np.sum(num) Q = np.maximum(Q, 1e-12) # Compute gradient PQ = P - Q for i in range(n): dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0) # Perform the update if iter \u0026lt; 20: momentum = initial_momentum else: momentum = final_momentum gains = (gains + 0.2) * ((dY \u0026gt; 0.) != (iY \u0026gt; 0.)) + \\ (gains * 0.8) * ((dY \u0026gt; 0.) == (iY \u0026gt; 0.)) gains[gains \u0026lt; min_gain] = min_gain iY = momentum * iY - eta * (gains * dY) Y = Y + iY Y = Y - np.tile(np.mean(Y, 0), (n, 1)) # Compute current value of cost function if (iter + 1) % 10 == 0: C = np.sum(P * np.log(P / Q)) print(\u0026quot;Iteration %d: error is %f\u0026quot; % (iter + 1, C)) # Stop lying about P-values if iter == 100: P = P / 4. # Return solution return Y if __name__ == \u0026quot;__main__\u0026quot;: print(\u0026quot;Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\u0026quot;) print(\u0026quot;Running example on 2,500 MNIST digits...\u0026quot;) X = np.loadtxt(\u0026quot;mnist2500_X.txt\u0026quot;) labels = np.loadtxt(\u0026quot;mnist2500_labels.txt\u0026quot;) Y = tsne(X, 2, 50, 20.0) np.savetxt(\u0026quot;myfile.txt\u0026quot;, Y) # data_to_show = np.loadtxt(\u0026quot;myfile.txt\u0026quot;) data_to_show = Y fig, ax = plt.subplots() scatter = plt.scatter(data_to_show[:, 0], data_to_show[:, 1], 50, labels, alpha=0.4) for i, txt in enumerate(labels[0:40]): ax.annotate(txt, (data_to_show[i, 0], data_to_show[i, 1])) legend1 = ax.legend(*scatter.legend_elements(), loc=\u0026quot;upper left\u0026quot;, title=\u0026quot;Digit\u0026quot;) ax.add_artist(legend1) ax.set_title('Visualize high-dimensional data points \\n in 2-dimensional plot by t-SNE', fontsize=16) ax.set_xlabel('Dimension 1') ax.set_ylabel('Dimension 2') plt.savefig('t-SNE.png') plt.show()  As can be seen in the following figure, the visualized high-dimensional mnist dataset is shown in the lower 2-dimensional space. 2.3 How to give the hyper parameters   n_components: Dimension of the embedded space, this is the lower dimension that we want the high dimension data to be converted to. The default value is 2 for 2-dimensional space. Perplexity: The perplexity is related to the number of nearest neighbors that are used in t-SNE algorithms. Larger datasets usually require a larger perplexity. Perplexity can have a value between 5 and 50. The default value is 30. n_iter: Maximum number of iterations for optimization. Should be at least 250 and the default value is 1000 learning_rate: The learning rate for t-SNE is usually in the range [10.0, 1000.0] with the default value of 200.0.   3. Comparation between PCA and t-SNE   The PCA is a linear algorithm which is not able to interpret complex ploynomial relationship between features. By contrast, t-SNE is actived based on probability distributions on neighbourhood graph to understand the structure with the data; The linear dimension reduction algorithm (PCA) concentrates on placing dissimilar data points far apart in a lower dismension representation, while the nonlinear algorithm (t-SNE) places the similar datapoints closely together in lower dimesional space. Hence, as can be seen in the following figure, PCA can only capture linear structures in the features. The t-SNE algorithm works in a very different way and focuses to preserve the local distances of the high-dimensional data in some mapping to low-dimensional data.    import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.manifold import TSNE X = np.loadtxt(\u0026quot;mnist2500_X.txt\u0026quot;) labels = np.loadtxt(\u0026quot;mnist2500_labels.txt\u0026quot;) ## implementing by PCA pca = PCA(n_components=2, svd_solver='full') data_to_show_PCA = pca.fit_transform(X) ## implementing by t-SNE data_to_show_tSNE = TSNE(n_components=2).fit_transform(X) ## Figure PCA fig, ax = plt.subplots(1,2,figsize=(20, 8)) scatter1 = ax[0].scatter(data_to_show_PCA[:, 0], data_to_show_PCA[:, 1], 50, labels, alpha=0.4) for i, txt in enumerate(labels[0:50]): ax[0].annotate(txt, (data_to_show_PCA[i, 0], data_to_show_PCA[i, 1])) ax[0].set_title('PCA', fontsize=16) ax[0].set_xlabel('Principal 1') ax[0].set_ylabel('Principal 2') legend1 = ax[0].legend(*scatter1.legend_elements(), loc=\u0026quot;upper left\u0026quot;, title=\u0026quot;Digit\u0026quot;) ax[0].add_artist(legend1) fig.suptitle('Visualize high-dimensional data points through \\n PCA and t-SNE', fontsize=16) ## Figure t-SNE scatter2 = ax[1].scatter(data_to_show_tSNE[:, 0], data_to_show_tSNE[:, 1], 50, labels, alpha=0.4) for i, txt in enumerate(labels[0:50]): ax[1].annotate(txt, (data_to_show_tSNE[i, 0], data_to_show_tSNE[i, 1])) ax[1].set_title('t-SNE', fontsize=16) ax[1].set_xlabel('Dimension 1') ax[1].set_ylabel('Dimension 2') legend2 = ax[1].legend(*scatter2.legend_elements(), loc=\u0026quot;upper left\u0026quot;, title=\u0026quot;Digit\u0026quot;) ax[1].add_artist(legend2) plt.savefig('PCA_t-SNE.png') plt.show()  References   A tutorial on Principal Components Analysis  Principal component analysis  PCA: A Practical Guide to Principal Component Analysis in R \u0026amp; Python  A simple introduction to PCA.  Chaper12: Continous latent variables in Pattern recognition and machine learning.  t-SNE Walkthrough  Good hyperparameter Information  Laurens van der Maaten personal page  Laurens van der Maaten\u0026rsquo; talk about t-SNE  What advantages does the t-SNE algorithm have over PCA?  ","date":1603369809,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603369809,"objectID":"d5d02082d29eb859326c553470b0d1be","permalink":"/post/t-sne/","publishdate":"2020-10-22T20:30:09+08:00","relpermalink":"/post/t-sne/","section":"post","summary":"Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) are all efficient ways to transform the data points in high dimesion to the corresponding feature points in low dimension without losing the principal feature.","tags":["PCA","t-SNE","Machine learning"],"title":"Dimension Reduction and High Dimensional Data Visualization by PCA and t-SNE","type":"post"},{"authors":["Yinyin SU"],"categories":[],"content":"In machine learning, the perceptron is an algorithm for supervised learning of binary classfiers. The perceptron algorithm was invented 1958 at the Cornell Aeronautical lab by Frank Rosenblatt.\nPerceptron model For input space (featured space) $\\mathcal{X} = {{\\mathbf{R}}^{n}}$ and output space $\\mathcal{Y} =\\{-1, +1\\}$, the perceptron model can be built by: $$f(x) = sign(\\omega \\centerdot x+b)$$ $$sign(x) = \\begin{cases} +1 \u0026amp; \\text{if } x \\ge 0,\\\\\n-1 \u0026amp; \\text{if } x \u0026lt; 0. \\end{cases}$$ where $\\omega$ and $b$ are the weight and bias for the above function respectively. Through being trained by the give training data $T = \\{ (x_0, y_0), (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\}$, where $x_i \\in \\mathcal{X} = {{\\mathbf{R}}^{n}}$ and $y_i \\in \\mathcal{Y}=\\{-1, +1\\}$, $i = 1, 2, \\cdots, N$. The parameters $\\omega$ and $b$ can be leanned through the mentioned data set. The learned linear hyperplane can divide the training data into two parts labelling +1 and -1. The schematic diagram is shown as follows.\n The schematic diagram of perceptron   Perceptron strategy In perceptron algorithm, the total distance to the hyperplane is defined to depict its cost function: $$L\\left( \\omega ,b \\right)=-\\sum\\limits_{{{x}_{i}}\\in M}{{{y}_{i}}\\left( \\omega \\cdot {{x}_{i}}+b \\right)}$$ where $M$ is misclassfication data set.\nPerceptron learning algorithm For the given training data set, the parameters $\\omega$ and $b$ can be optimized by minimizing the cost function: $$\\underset{\\omega ,b}{\\mathop{\\min }},L\\left( \\omega ,b \\right)=-\\sum\\limits_{{{x}_{i}}\\in M}{{{y}_{i}}\\left( \\omega \\cdot {{x}_{i}}+b \\right)}$$ We can use stochastic gradient descent method to solve the above optimized problem. Take partial derivative with respect to $\\omega$ and $b$, the iteration strategy can be obtained: $$\\begin{align} \u0026amp; \\omega \\leftarrow \\omega +\\eta {{y}_{i}}{{x}_{i}} \\\n\u0026amp; b\\leftarrow b+\\eta {{y}_{i}} \\\n\\end{align}$$ where $\\eta$ is a learning rate hyperparameter set by user.\nBased on the aforementioned discussion, the perceptron algorithm can be derived: $\\omega$, $b$\n  Preceptron algorithm   Input: training data set: $T = { (x_0, y_0), (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)}$, $i = 1, 2, 3, \\cdots, N$; learning rate $\\eta (0 \u0026lt; \\eta \\le 1)$; Output: $\\omega$, $b$ and preceptron model: $f(x) = sign(\\omega \\centerdot x+b)$ Initialize $\\omega_0, b_0$; while: Choosing the single instance $(x_i, y_i)$ from $T$ until no misclassfication data contained in the set; â€ƒif $y_i \\left( \\omega \\cdot x_i +b\\right) \\le 0$ â€ƒ$\\omega \\leftarrow \\omega +\\eta {y_i}{x_i}$ and $b\\leftarrow b+\\eta {{y}_{i}}$ â€ƒendif end while   Implementing the algorithm in python3 from matplotlib import pyplot as plt import numpy as np from matplotlib.animation import FuncAnimation # define a function to realize preceptron algorithm. def PrecessingPreceptron(point, learning_rate): w_set = [] b_set = [] w = np.array([0,0]) b = 0 flag = 1 while flag: flag = 0 i = 0 for i in range(len(point)): if point[i,2]*(np.matmul(w, point[i,0:2]) + b) \u0026lt;= 0: w = w + learning_rate * point[i,0:2] * point[i,2] b = b + learning_rate * point[i,2] w_set.append(w) b_set.append(b) flag = 1 break if flag == 0: break return w_set, b_set # DrawFigure is used to draw the process def DrawFigure(count, w_set, b_set, gap): point_x_1 = 0 point_x_2 = 5 if count == 0: line.set_data([], []) return line, if gap*count \u0026lt; len(w_set): w = w_set[count*gap] b = b_set[count*gap] else: w = w_set[-1] b = b_set[-1] if w[1] != 0: point_y_1 = -(w[0] * point_x_1 + b) / w[1] point_y_2 = -(w[0] * point_x_2 + b) / w[1] thisx = np.array([point_x_1, point_x_2]) thisy = np.array([point_y_1, point_y_2]) line.set_data(thisx, thisy) return line, if __name__ == \u0026quot;__main__\u0026quot;: # define input data set traning_data = np.array([[3, 4, 1],[4, 3, 1], [3,3,-1],[1,2,-1],[3,5,1],[4,7,1],[3,2,-1],[1.5,5,-1]]) learning_rate = 0.7 fig, ax = plt.subplots() line, = ax.plot([], [], 'k-') for index in range(len(traning_data)): if traning_data[index,2] == 1: ax.plot(traning_data[index, 0], traning_data[index, 1], 'ro') else: ax.plot(traning_data[index,0], traning_data[index,1],'bo') plt.xlabel('$x_1$') plt.ylabel('$x_2$') plt.title('Preceptron algorithm learning precess') w_set, b_set = PrecessingPreceptron(traning_data, learning_rate) anim = FuncAnimation(fig, DrawFigure,frames=np.arange(0, 15), fargs=(w_set, b_set, 50), interval=300, blit=True) anim.save('preceptron.gif', writer='imagemagick') plt.show()  Results: Notes   It can be testified that the perceptron algorithm could converge at last within the limited steps of iterations. The original algorithm have multiple solutions coming from: 1. The initial parameters $\\omega_0, b_0$; 2. the selection order of misclassficated point in the learning process.   ","date":1599136209,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599136209,"objectID":"80f24f9802944cd6d9522547225bd036","permalink":"/post/preceptron/","publishdate":"2020-09-03T20:30:09+08:00","relpermalink":"/post/preceptron/","section":"post","summary":"In machine learning, the perceptron is an algorithm for supervised learning of binary classfiers. The perceptron algorithm was invented 1958 at the Cornell Aeronautical lab by Frank Rosenblatt.\nPerceptron model For input space (featured space) $\\mathcal{X} = {{\\mathbf{R}}^{n}}$ and output space $\\mathcal{Y} =\\{-1, +1\\}$, the perceptron model can be built by: $$f(x) = sign(\\omega \\centerdot x+b)$$ $$sign(x) = \\begin{cases} +1 \u0026amp; \\text{if } x \\ge 0,\\\\","tags":["Preceptron","Machine learning"],"title":"Preceptron algorithm","type":"post"},{"authors":["Yinyin SU"],"categories":[],"content":"ROS is an open-source, meta-operating system for your robot. Some advantages are as follows.\n Distributed computation Software resure Rapid testing  Packages rospack list # You can obtain a list of all of the installed ROS packages. rospack find package-name # find the directory of a single package rosls package-name # view all files in the package directory roscd package-name # go to the package directory directly  Nodes ROS node communication network  ROS node communication network   rosrun package-name executable-name # Starting a node rosnode list #listing nodes rosnode info node-name # get some information abour a particular node. rosnode kill node-name # Killing a node. rosnode cleanup #remove the dead nodes from the list. rqt_graph # veiwing the graph rosmsg show message-type-name #inspecting a message type. rostopic pub -r rate-in-hz topic-name message-type message-content # publishing messages from the command line. rosrun turtlesim turtlesim_node __name:=A rosrun turtlesim turtlesim_node __name:=B # run the same node with different name, as ROS master does not allow multiple nodes with the same name. roswtf # perform a broad variety of sanity checks.  Creating a workplace and a package  The file structure of workplace   user@hostname$ mkdir -p ~/catkin_ws/src user@hostname$ cd ~/catkin_ws/src user@hostname$ catkin_init_workspace user@hostname$ source devel/setup.bash user@hostname$ cd ~/catkin_ws/src user@hostname$ catkin_create_pkg my_package_name # creating two default versions of these two configuration files: package.xml and CMakeLists.txt.  The first program \u0026lsquo;hello world\u0026rsquo; in ROS  Step 1: Write the helle world program  // This is a ROS version fo the standard \u0026quot;hello world\u0026quot; program // This header defines the standard ROS classes. #include \u0026lt;ros/ros.h\u0026gt; int main(int agrc, char** agrv){ //Initialize the ROS system. ros::init(argc, argv, \u0026quot;hello_ros\u0026quot;); // Establish this program as a ROS node ros::NodeHandle nh; //Send some output as a log message. ROS_INFO_STREAM(\u0026quot;Hello, ROS!\u0026quot;); return 0; }   Step 2: Compiling the hello world program    Declaring dependencies   CMakeLists.txt find_package(catkin REQUIRED COMPONENTS package-names) package.xml \u0026lt;build_depend\u0026gt;package-name\u0026lt;/build_depend\u0026gt; \u0026lt;run_depend\u0026gt;package-name\u0026lt;/run_depend\u0026gt;   Declaring an executable   CMakeLists.txt add_executable(executable-name source-files1 source-files2 ...) target_link_libraries(executable-name ${catkin_LIBRARIES})   Building the workplace catkin_make source devel/setup.bash   Write a publisher node and a subscriber node  Be mindful of the lifetime of your ros::Publisher objects. Creating the publisher is an expensive operation, so it\u0026rsquo;s a usually a bad idea to creat a new ros::Publisher object each time you want to publish a message. Refer the ROS wiki Publisher.  // This program publishes randomly-generated velocity messages for turtlesim. #include\u0026lt;ros/ros.h\u0026gt; // incldue message type declaration #include\u0026lt;geometry_msgs/Twist.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; int main(int agrc, char** agrv){ //Initialize the ROS system and become a node ros::init(agrc, agrv, \u0026quot;publish_velocity\u0026quot;); ros::NodeHandle nh; //creat a publisher object // ros::Publisher pub = node_handle.advertise\u0026lt;message_type\u0026gt;(topic_name, queue_size); // queue size is an integer representing the size of the message queue for publisher. In most case, a reasonablly large value, say 1000, is suitable. ros::Publisher pub = nh.advertise\u0026lt;geometry_msgs::Twist\u0026gt;(\u0026quot;turtle1/cmd_vel\u0026quot;, 1000); //send the random number generator. srand(time(0)); //loop at 2 Hz until the node is shut down. ros::Rate rate(2); // ros::ok() function is used to check for node shutdown. while(ros::ok()){ // creat and fill in the message. The other four fields, which are ignored by turtlesim, default to 0. geometry_msgs::Twist msg; msg.linear.x = double(rand())/double(RAND_MAX); msg.linear.z = 2*double(rand())/double(RAND_MAX) - 1; //Publish the message. pub.publish(msg); //send a message to rosout with the details. ROS_INFO_STREAM(\u0026quot;Sending random velocity command: \u0026quot;\u0026lt;\u0026lt; \u0026quot; linear=\u0026quot; \u0026lt;\u0026lt; msg.linear.x \u0026lt;\u0026lt; \u0026quot; angular=\u0026quot; \u0026lt;\u0026lt; msg.angular.z); //Wait until it's time for another iteration. rate.sleep(); } return 0; }   Refer to ROS wiki Subscriber  //This program subscribes to turtle 1 /pose and show its message on the sreen. #include\u0026lt;ros/ros.h\u0026gt; #include\u0026lt;turtlesim/ Pose.h\u0026gt; #include\u0026lt;iomanip\u0026gt; // for std::setprecision and std::fixed //A callback function. Excuted each time a new pose // message arrives. void poseMessageReceived(const turtlesim::Pose\u0026amp; msg){ ROS_INFO_STREAM(std::setprecision(2) \u0026lt;\u0026lt; std::fixed \u0026lt;\u0026lt; \u0026quot;position =(\u0026quot; \u0026lt;\u0026lt; msg.x \u0026lt;\u0026lt; \u0026quot;, \u0026quot; \u0026lt;\u0026lt; msg.y \u0026lt;\u0026lt; \u0026quot;)\u0026quot; \u0026lt;\u0026lt; \u0026quot; direction=\u0026quot; \u0026lt;\u0026lt; msg.theta); int main(int agrc, char** agrv){ // Initialize the ROS system and become a node. ros::init(agrc, agrv, \u0026quot;subscribe_to_pose\u0026quot;); ros::NodeHandle nh; //creat a subcriber object // ros::Subscriber sub = node_handle.subscribe(topic_name, queue_size, pointer_to_callback_function); //Most of parameters have analogues in the declaration of a ros::Publisher object. ros::Subscriber sub = nh.subscribe(\u0026quot;turtle1/pose\u0026quot;. 1000, \u0026amp;poseMessageReceived); //let ROS take over. ros::spin(); } }    The relationship between ros::spin() and ros::spinonce()    ROS will only execute our callback function when we give it explicit permission to do so by using ros::spin() or ros::spinonce() ros::spin() will be called all the time, while ros::spinonce() is called only one time. Hence, the main process will block when you use ros::spin() function until the the node shutdown. ros::spin() equals to  while(ros::ok()){ ros::spinonce(); }   If you want to subscribe and publish message using the same node, you can refer to this example by Dhruv Ilesh Shah.   ","date":1597131261,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597131261,"objectID":"76b0827d471e537d55e48ae496e0d489","permalink":"/post/someusefulcommandinros/","publishdate":"2020-08-11T15:34:21+08:00","relpermalink":"/post/someusefulcommandinros/","section":"post","summary":"ROS is an open-source, meta-operating system for your robot. Some advantages are as follows.\n Distributed computation Software resure Rapid testing  Packages rospack list # You can obtain a list of all of the installed ROS packages.","tags":["ROS","Ubuntu"],"title":"Some useful commands for ROS in Ubuntu","type":"post"},{"authors":["Yinyin SU","Zhonggui Fang","Wenpei Zhu","Xiaochen Sun","Yuming Zhu","Hexiang Wang","Hailin Huang","Sicong Liu","Zheng Wang"],"categories":null,"content":" The file structure of workplace   --  Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   -- ","date":1581465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581465600,"objectID":"29cddef5cb35112173080c6ccebea2f5","permalink":"/publication/journal-article-1/","publishdate":"2020-02-12T00:00:00Z","relpermalink":"/publication/journal-article-1/","section":"publication","summary":"Hybrid robotic gripper, soft origamic actuators, POSA joint, higher force capability, proprioception.","tags":["proprioception","soft origamic actuators"],"title":"A High-Payload Proprioceptive Hybrid Robotic Gripper With Soft Origamic Actuators","type":"publication"},{"authors":["Qi Kang","Jia Wang","Li Duan","Yinyin SU","Jianwu He","Di Wu","Wenrui Hu"],"categories":null,"content":" The file structure of workplace   --  Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   -- ","date":1555459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"d1f0f41091b62eaace3490eb556fa330","permalink":"/publication/journal-article-3/","publishdate":"2019-04-17T00:00:00Z","relpermalink":"/publication/journal-article-3/","section":"publication","summary":"Marangoni convection, pattern formation, parametric instability.","tags":["Microgravity fluid","Shijian-10"],"title":"The volume ratio effect on flow patterns and transition processes of thermocapillary convection","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"Display some concact information about me!","tags":null,"title":"Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"/experience/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"Hello!","tags":null,"title":"Experiences","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"Display some concact information about me!","tags":null,"title":"Projects","type":"widget_page"},{"authors":["Yinyin SU","Yuquan Wang","Abderrahmane Kheddar"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"d3b976677055f6238fa12f54d25dc9ef","permalink":"/publication/conference-paper-1/","publishdate":"2019-01-24T00:00:00Z","relpermalink":"/publication/conference-paper-1/","section":"publication","summary":"Task analysis, Optimization,Bayes methods, Robots, Tuning, Linear programming.","tags":["Bayesian optimization","Gaussian process"],"title":"Sample-Efficient Learning of Soft Task Priorities Through Bayesian Optimization","type":"publication"},{"authors":["Yinyin SU","Di Wu","Li Duan","Qi Kang","Peishi Lyu","Sheng Xu","Chunfeng Lao","Huacheng Song","Jingjing Zhang"],"categories":null,"content":" The file structure of workplace   --  Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   -- ","date":1514851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514851200,"objectID":"0178ebac3243c0da1a0e147a0926fb90","permalink":"/publication/journal-article-2/","publishdate":"2018-01-02T00:00:00Z","relpermalink":"/publication/journal-article-2/","section":"publication","summary":"micro-gravity environment; centrifugal cone-shaped two-phase washing machine; VOF method; micro-gravity fluid management; numerical simulation.","tags":["Microgravity washing machine"],"title":"Numerical Simulation of Flow Field in Centrifugal Cone-shaped Two-phase Washing Machine under Microgravity","type":"publication"},{"authors":["Yongqiang Li","Mingzhu Hu","Ling Liu","Yinyin SU","Li Duan","Qi Kang"],"categories":null,"content":" The file structure of workplace   --  Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   -- ","date":1434585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1434585600,"objectID":"bf92a6b924ce657024ce9f5f097404f9","permalink":"/publication/journal-article-4/","publishdate":"2015-06-18T00:00:00Z","relpermalink":"/publication/journal-article-4/","section":"publication","summary":"Rounded wall, Capillary driven flow, Approximate analytical solution, Liquidâ€™s front position.","tags":["Microgravity fluid"],"title":"Study of Capillary Driven Flow in an Interior Corner of Rounded Wall under Microgravity","type":"publication"}]