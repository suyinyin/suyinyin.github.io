[{"authors":["admin"],"categories":null,"content":"I am a PhD Student of control and robotics at the Automatic Control Lab supervised by Professor James Lam and at the Biorobotic and Control Lab supervised by Professor Zheng Wang respectively. My research interests include soft robotics, applying control and optimization algorithms in robot, robotic system and kinematic and dynamic of robot.\nI like playing basketball and football. Lebron James and Antoine Griezmann are my favorite basketball and football player, respectively. Besides, I am interested in fishing and grasp some skills in ice skating and skiing. If you have the same hobbies or interests in my research field, please do not hesitate to contact me.\n","date":1608294609,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1608294609,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/yinyin-su/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yinyin-su/","section":"authors","summary":"I am a PhD Student of control and robotics at the Automatic Control Lab supervised by Professor James Lam and at the Biorobotic and Control Lab supervised by Professor Zheng Wang respectively.","tags":null,"title":"Yinyin SU","type":"authors"},{"authors":["Yinyin SU"],"categories":[],"content":"Recently, 3D cameras have been widely used in various computer vision applications, like robotics, autonomous driving. Leveraging the extra data provided by such sensors allows for better performance on tasks such as detection and recognition, pose estimation, 3D reconstruction, and so forth. The post presented some common knowledge about the 3D cameras, including the overview of the most common 3D sensing techniques on the markets and their underlying mechanisms, some camera calibration technology, hand-eye calibration methods, mapping depth to point cloud.\n1. 3D camera technique At present, the three prevalent 3D imaging technologies are binocular stereo vision, time of flight (ToF) and structured light. Concerning active light source projection, the techniques fall into passive and active categories. Binocular stereo vision is a passive technique and the other two are active techniques.\n1.1. Bibocular stereo vision Computer stereo vision is the extraction of 3D information from digital images, such as those obtained by CCD cameras. By comparing the information about a scene from two vantage points, 3D information can be extracted by examining the relative position of objects in the two panels. This skill mimics the binocular vision of human beings. Humans have two eyes from which there is about 60mm to 70mm apart. It will result in slight image location disparity when the eyes view the same scene. The stereo vision which is the same as the human eye needs two lenses, enabling each of them to capture these slightly different images. Based on this disparity, the depth information can be computed. It is a passive technique because no external light is required other than the ambient light, which means it is suitable for outdoor use in relatively good light conditions. The stereo matching method of this technique requires the great processing power of the sensor to guarantee resolution and instantaneous output. Constrained by the baseline, this skill often works in a short range, often within 2 meters. Famous stereo cameras include ZED 2K Stereo camera of sterols and Point grey\u0026rsquo;s BumbleBee.\nBibocular stereo visio 1.2. ToF ToF is a technique of calculating the distance between the camera and the object, by measuring the time it takes the projected infrared light to travel from the camera, bounced off the object surface, and return to the sensor. Due to the constant light speed, the processor can calculate the distance of the object and reconstruct it by analyzing the phase shift of the emitted and returned light. Unlike the stereo vision technology, ToF is an active technique, as it actively projects light to measure the distance, instead of relying on ambient light. It works well in dim or dark light conditions. ToF cameras are widely applied in the field of VR/AR, robotic navigation, objective recognition, and auto piloting. Well-known products are a series of Kinect depth cameras produced by Microsoft.\nTime of Flight 1.3. Structured light The structured light technique is another active 3D imaging method, which is very similar to the stereo vision technique on basic mechanisms. Different from the stereo vision method, it employs structured light without depending on external light conditions. The cameras project modulated pattern to the surface of a scene and calculate the disparity between the original projected pattern and the observed pattern deformed by the surface of the scene. As an active skill, structured light cameras can work well in conditions lacking light or texture as well. Compared with the ToF method, the well-modulated light pattern can generate higher accuracy in the short range. And the depth resolution can reach the submillimeter level. Intel adopts a structured light technique in the Realsense depth camera series. Structured light cameras are proper for cases requiring high accuracies in short-range, such as face recognition, gesture recognition, and industrial inspection.\nThe structured light 1.4. Pros and cons 2. Camera calibaration  Camera calibration is a necessary step in 3D computer vision to extract metric information from 2D images. Calibration aims to find the quantities internal to the cameras that affect the image process, including image center, focal length, lens distortion parameters. The precise internal camera parameters are of paramount importance for the 3D interpretation of images, reconstruction of world models, and robot interaction with the world.\n2.1. Pinhole camera model  The pinhole camera model is a model of an ideal camera, that describes the mathematical relationship between the real world 3D object\u0026rsquo;s coordinates and its 2D projection on the image plane . Its validity depends on the quality of the camera and, in general, decreases from the center of the image to the edges as lens distortion effects increase. As shown in the above figure, if we want to understand the relation between the 3D objects and the corresponding 2D images, the mathematical model is needed to describe the relationship. In this post, we use $(\\textbf U, \\textbf V, \\textbf W)$, $( \\textbf X, \\textbf Y, \\textbf Z)$, $(x, y)$, and $(u, v)$ to depict the positions of an object in real-world space, camera space, film space, and pixel space respectively. Firstly, we will build a model to map objects from camera coordinates to film coordinates. As shown in the above figure, the $(x, y)$ in the film space can be derived from the point in camera space via similar triangles rule. $f$ is the focal length. It is worth to be mentioned that the focal length along the x-direction, y-direction can be different, and it can be depicted by $f_x$, and $f_y$. The origin of the image in film space is at its center, while the origin of the pixel space is at the top-left position. So the offsets are needed to transfer the points in film space to those in pixel points. $o_x$ and $o_y$ are called $x$ offset and $y$ offset respectively. The above two processes often are put together, which can be described by a $3\\times3$ matrix $\\mathbf K$. The parameters in $\\mathbf K$ is the intrinsic parameters of the camera. It is different from different devices. It is recommended that these parameters should be known before you use a camera. The variance of data set $\\mathbf{z_1}$\n$$\\mathbf{K} = {\\left[ \\begin{matrix} f_x \u0026amp; 0 \u0026amp; o_{x} \\\\\n0 \u0026amp; f_{y} \u0026amp; {{o}_{y}} \\\\\n0 \u0026amp; 0 \u0026amp; 1 \\\\\n\\end{matrix} \\right]}$$\nSo the pixel coordiante value can be obtained by: $${{\\left[ \\begin{matrix} u \u0026amp; v \u0026amp; 1 \\\n\\end{matrix} \\right]}^{T}}=\\mathbf{K}{{\\left[ \\begin{matrix} {X}/{Z}; \u0026amp; {Y}/{Z}; \u0026amp; 1 \\\n\\end{matrix} \\right]}^{T}}.$$ Mapping objects from real-world space to camera space could use a homogeneous transformation matrix $\\mathbf T$, which is a common approach to describe the body transformation in 3D space. $$\\mathbf T = \\left[ \\begin{matrix} {{r}_{11}} \u0026amp; {{r}_{12}} \u0026amp; {{r}_{13}} \u0026amp; {{t}_{x}} \\\\\n{{r}_{21}} \u0026amp; {{r}_{22}} \u0026amp; {{r}_{23}} \u0026amp; {{t}_{y}} \\\\\n{{r}_{31}} \u0026amp; {{r}_{23}} \u0026amp; {{r}_{33}} \u0026amp; {{t}_{z}} \\\\\n0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\\n\\end{matrix} \\right]$$ By combining all the mentioned transform matrix, the objects in 3D real-world space can be transferred into the 2D image space, as shown in the following figure. The way in how to calibrate the internal and external parameters of the camera will be presented in the next subsection 3.\nPinhole camera model 2.2 Lens Distortion Model Most cameras on the market are made up of convex lens to capture light, which brings some issues to be addressed: 1. have finite aperture so a blurring of unfocused objects appears; 2. contain geometric distortions due to lenses, which increase as we get closer to the edges of the lenses. The most common type of camera lens distortion is called radial distortion, including positive or barrel radial distortion and negative or pincushion radial distortion, as depicted in the below figure.\nRadial distortion of lens camera Some action cameras which have large FOV will cause a lot of positive radial distortion. The negative radial distortion is often caused by the lens being not aligned perfectly parallel to the imaging plane. It leads to the image look tilted, which is bad for us since some objects look further away than they are. Next, we will discuss how to avoid the radial distortion, and two couples of coefficients $k_i$, describing radial distortion and $p_i$, describing the tangential distortion. The worse the distortion, the more coefficients we need to accurately describe it. More details about these parameters were given in OpenCV docs. In the pinhole camera model, we use intrinsic matrix $\\mathbf K$ to map object in camera coordinate to image in pixel coordinate. Expanding matrix $\\mathbf K$, the following equations can be derived. $$\\begin{align} \u0026amp; x=X/Z \\nonumber \\\\\n\u0026amp; y=Y/Z \\nonumber \\\\\n\u0026amp; u={{f}_{x}}\\cdot x+{{o}_{x}} \\nonumber \\\\\n\u0026amp; v={{f}_{y}}\\cdot y+{{o}_{y}} \\nonumber \\\\\n\\end{align}$$ Due to the lens distortion in the real application of cameras, the distortion parameters should be added to the model. After getting point $(\\textbf X, \\textbf Y, \\textbf Z)$ in camera space, the updated image pixel coordinates along $u$ direction and $v$ direction can be obtained: $${x}\u0026lsquo;=x\\frac{1+{{k}_{1}}{{r}^{2}}+{{k}_{2}}{{r}^{4}}+{{k}_{3}}{{r}^{6}}}{1+{{k}_{4}}{{r}^{2}}+{{k}_{5}}{{r}^{4}}+{{k}_{6}}{{r}^{6}}}+2{{p}_{1}}xy+{{p}_{2}}$$\n$${y}\u0026lsquo;=y\\frac{1+{{k}_{1}}{{r}^{2}}+{{k}_{2}}{{r}^{4}}+{{k}_{3}}{{r}^{6}}}{1+{{k}_{4}}{{r}^{2}}+{{k}_{5}}{{r}^{4}}+{{k}_{6}}{{r}^{6}}}+{{p}_{1}}({{r}^{2}}+2{{y}^{2}})+2{{p}_{2}}xy$$\nwhere ${{r}^{2}}={{x}^{2}}+{{y}^{2}}$, $u={{f}_{x}}\\cdot {x}\u0026lsquo;+{{o}_{x}}$ and $v={{f}_{y}}\\cdot {y}\u0026lsquo;+{{o}_{y}}$. Since we\u0026rsquo;re primarily interested in efficiently removing the radial distortion, we\u0026rsquo;ll be using Fitzgibbon\u0026rsquo;s division model as opposed to Brown-Conrady\u0026rsquo;s even-order polynomial model, since it requires fewer terms in cases of severe distortion. It is also a bit easier to work with since inverting the single parameter division model requires solving one degree less polynomial than inverting the single-parameter polynomial model.\n2.3. A Flexible New Technique for Camera In this subsection, we will talk about some popular camera calibration techniques. In terms of the calibrated object used in camera calibration, four major methods have been applied in different fields: calibration using 3D calibration object, calibrating using the 2D planar pattern, calibration using the 1D object (line-based calibration), and self-calibration, which is no calibration objects. For the 3D method, calibration is performed by observing a calibration object whose geometry in 3D space is known with very good precision. The object often contains two or three orthogonal to each other, e.g. calibration cube, and it has a plane undergoing a precisely known translation. Although the 3D method is an expensive and more elaborate setup, it is more accurate and has a simple theory. The 2D plan-base calibration requires observation of a planar pattern shown at a few different orientations. Mostly, a popular planar pattern is a checkerboard. Owing to the easy setup, the 2D method is the most popular one. In this post, we concentrate on the 2D plan-based calibration method. You can refer to the lectures of Ahmed Elgammal for other methods and their comparisons. Here, we will pay more attention to the contributions in A Flexible New Technique for Camera presented by Zhengyou Zhang, which is the most popular 2D calibration approach. Firstly, setting the world coordinate system to the corner of the checkerboard, and axis z is perpendicular outward to the checkerboard. Due to all points lying in a plane, the z value of all points in the checkerboard is zero, namely $ W = 0$ . So the 3rd Colum of the extrinsic matrix will vanish, and the camera model becomes: $$\\lambda \\left[ \\begin{matrix} u \\\\\nv \\\\\n1 \\\\\n\\end{matrix} \\right]=\\underbrace{\\left[ \\begin{matrix} {{f}_{x}} \u0026amp; 0 \u0026amp; {{o}_{x}} \\\\\n0 \u0026amp; {{f}_{y}} \u0026amp; {{o}_{y}} \\\\\n0 \u0026amp; 0 \u0026amp; 1 \\\\\n\\end{matrix} \\right]\\left[ \\begin{matrix} {{r}_{11}} \u0026amp; {{r}_{12}} \u0026amp; {{t}_{x}} \\\\\n{{r}_{21}} \u0026amp; {{r}_{22}} \u0026amp; {{t}_{y}} \\\\\n{{r}_{31}} \u0026amp; {{r}_{23}} \u0026amp; {{t}_{z}} \\\\\n\\end{matrix} \\right]}_{\\mathbf{H}}\\left[ \\begin{matrix} U \\\\\nV \\\\\n1 \\\\\n\\end{matrix} \\right], $$\nwhere $\\mathbf H$ is the $3\\times3$ matrix, containing 9 parameters to be addressed. As the third element in the left side vector, actually, it needs 8 equations to solve all unknown elements. For the checkerboard, at least 4 points are needed to be chosen.\n$$\\mathbf{H}=\\left( {{\\mathbf{h}}_{\\mathbf{1}}}\\mathbf{,}{{\\mathbf{h}}_{\\mathbf{2}}}\\mathbf{,}{{\\mathbf{h}}_{\\mathbf{3}}} \\right)\\underbrace{\\left[ \\begin{matrix} {{f}_{x}} \u0026amp; 0 \u0026amp; {{o}_{x}} \\\\\n0 \u0026amp; {{f}_{y}} \u0026amp; {{o}_{y}} \\\\\n0 \u0026amp; 0 \u0026amp; 1 \\\\\n\\end{matrix} \\right]}_{\\mathbf{K}}\\underbrace{\\left[ \\begin{matrix} {{r}_{11}} \u0026amp; {{r}_{12}} \u0026amp; {{t}_{x}} \\\\\n{{r}_{21}} \u0026amp; {{r}_{22}} \u0026amp; {{t}_{y}} \\\\\n{{r}_{31}} \u0026amp; {{r}_{23}} \u0026amp; {{t}_{z}} \\\\\n\\end{matrix} \\right]}_\\mathbf{R}$$\nwhere $\\mathbf R={\\left[{{\\mathbf{r}}_{\\mathbf{1}}}\\mathbf{,}{{\\mathbf{r}}_{\\mathbf{2}}}\\mathbf{,t} \\right]}$.So $\\mathbf H$ can be described by:\n$$\\mathbf{H}=\\left( {{\\mathbf{h}}_{\\mathbf{1}}}\\mathbf{,}{{\\mathbf{h}}_{\\mathbf{2}}}\\mathbf{,}{{\\mathbf{h}}_{\\mathbf{3}}} \\right)=\\mathbf{K}\\left( {{\\mathbf{r}}_{\\mathbf{1}}}\\mathbf{,}{{\\mathbf{r}}_{\\mathbf{2}}}\\mathbf{,t} \\right), $$\nso ${{\\mathbf{r}}_{\\mathbf{1}}}={{\\mathbf{K}}^{-1}}{{\\mathbf{h}}_{\\mathbf{1}}}$ and ${{\\mathbf{r}}_{2}}={{\\mathbf{K}}^{-1}}{{\\mathbf{h}}_{2}}$. Because $\\mathbf r$ is rotarion matrix, so\n$$\\mathbf{r}_{1}^{T}{{\\mathbf{r}}_{2}}=0$$\n$$\\left| {{\\mathbf{r}}_{1}} \\right|=\\left| {{\\mathbf{r}}_{2}} \\right|=1$$\nThe constraint equations can be derived: $${{\\mathbf{h}}_{\\mathbf{1}}}{{\\mathbf{K}}^{-T}}{{\\mathbf{K}}^{-1}}{{\\mathbf{h}}_{2}}=0$$\n$${{\\mathbf{h}}_{\\mathbf{1}}}{{\\mathbf{K}}^{-T}}{{\\mathbf{K}}^{-1}}{{\\mathbf{h}}_{1}}-{{\\mathbf{h}}_{2}}{{\\mathbf{K}}^{-T}}{{\\mathbf{K}}^{-1}}{{\\mathbf{h}}_{2}}=0$$\nDefining $\\mathbf{B}:={{\\mathbf{K}}^{-T}}{{\\mathbf{K}}^{-1}}$, which is symmetric and positive definite matrix. $\\mathbf K$ can be calculated from $\\mathbf B$ using Cholesky factorization. $\\mathbf B$ can be depicted in the following: $$\\mathbf{B}=\\left[ \\begin{matrix} {{b}_{11}} \u0026amp; {{b}_{12}} \u0026amp; {{b}_{13}} \\\\\n{{b}_{12}} \u0026amp; {{b}_{22}} \u0026amp; {{b}_{23}} \\\\\n{{b}_{13}} \u0026amp; {{b}_{23}} \u0026amp; {{b}_{33}} \\\\\n\\end{matrix} \\right], $$\nso we can define a vector $\\mathbf b$ to represent $\\mathbf B$ :\n$$\\mathbf{b}={{\\left[ {{b}_{11}},{{b}_{12}},{{b}_{13}},{{b}_{22}},{{b}_{23}},{{b}_{33}} \\right]}^{T}}$$\nWe rewrite the constraint equations by $\\mathbf b$ in the following type: $$\\mathbf{v}_{ij}^{T}\\mathbf{b}=0$$\nwhere ${{\\mathbf{v}}_{ij}}={{\\left[ {{\\mathbf{h}}_{i1}}{{\\mathbf{h}}_{j1}},{{\\mathbf{h}}_{i1}}{{\\mathbf{h}}_{j2}}+{{\\mathbf{h}}_{i2}}{{\\mathbf{h}}_{j1}},{{\\mathbf{h}}_{i2}}{{\\mathbf{h}}_{j2}},{{\\mathbf{h}}_{i3}}{{\\mathbf{h}}_{j1}}+{{\\mathbf{h}}_{i1}}{{\\mathbf{h}}_{j3}},{{\\mathbf{h}}_{i3}}{{\\mathbf{h}}_{j2}}+{{\\mathbf{h}}_{i2}}{{\\mathbf{h}}_{j3}},{{\\mathbf{h}}_{i3}}{{\\mathbf{h}}_{j3}} \\right]}^{T}}$. So the updated type of constraint equations are $$\\left[ \\begin{matrix} \\mathbf{v}_{12}^{T} \\\\\n\\mathbf{v}_{11}^{T}-\\mathbf{v}_{22}^{T} \\\\\n\\end{matrix} \\right]\\mathbf{b}=0$$\nTo solve the above equation, at least 3 different images should be input. Owing to noise in the real measurement, the above equation can be solved in optimization methods by inputting more than 3 images. Solving $\\mathbf b$ via minimizing the following equation: $$\\mathbf{b}=\\arg \\underset{\\mathbf{b}}{\\mathop{\\min }}\\mathbf{v}^{T} \\mathbf {b}$$\nMore discussion about how to obtain the parameters after adding the lens distortion effects into the camera model can be found in Zhang\u0026rsquo;s paper.\n2.4 Depth camera sensor calibration Will be added.\n  CALIBRATION OF DEPTH CAMERAS USING DENOISED DEPTH IMAGES  Calibration of Depth Camera Arrays  Calibration using a general homogeneous depth camera model  The Depth I: Stereo Calibration and Rectification  2.5 Implementing calibration algorithm through OpenCV library and Matlab Before running the calibration program, you need to choose which kind of input where a camera, video and images are supplied to use in .\\calibration\\configurations.xml . The calibrated results are displayed in the .\\calibration\\out_camera_data.xml. The main function is shown as follow, the complete camera calibration program was added to gitlab repo. Also, you can use matlab to do the same task.\nint main(int argc, char* argv[]) { // 1. Read the settings from the configuration.xml while(){ // 2. Get the next image from the image list, camera or video file. // 3. Find the pattern in the current input // 4. Press the key: u - toggle the distortion removal, g - start again the detection process, ESC - end this application } // 5. save the calibrated results to out_camera_data.xml. return 0; }  3. Hand-eye calibration  In robotics and mathematics, the hand eye calibration problem (also called the robot-sensor or robot-world calibration problem) is the problem of determining the transformation between a robot end-effector and a camera or between a robot base and the world coordinate system. It has been widely used in vision-based robot control also known as visual servoing, which uses visual information from the camera as feedback to plan and control. ALl such application require accurate hand-eye calibration primarily to complement the accurate robotic arm pose with the sensor-based mearsurement of the observed environment into a more complete set of information. Hand‚Äìeye calibration requires accurate estimation of the homogenous transformation between the robot hand/end-effector and the optical frame of the camera affixed to the end effector. The problem can be formulated as $ùë®ùëø = ùëøùë©$, where $ùë®$ and $ùë©$ are the robotic arm and camera posesbetween two successive time frames, respectively, and $ùëø$ is the unknown transform between the robot hand (end effector) and the camera. In this post, we introduced two types of hand-eye calibration: eye-to-hand way and eye-in-hand way, as shown as follows: 3.1 Eye-to-hand intallment As shown in above figure, this is the eye-to-hand type installment, that is, the camera is installed in the fixed position from which the base of robotic arm has a constant relative position.\n {$\\textbf B$} \u0026ndash; The base coordinate system of robotic arm\n{$\\textbf E$} \u0026ndash; The end-effector coordinate system\n{$\\textbf S$} \u0026ndash; The checkerboarder coordinate system\n{$\\textbf C$} \u0026ndash; The camera coordinate system (generally the RGB sensor coordinate system)\n From base coordinate ${\\text B }$ to camera coordinate ${\\text C }$, the homogeneous tranformation matrix can be derived as follows: $${{T}_{\\text{BC}}}=T_{\\text{BE}}^{1}\\cdot T_{\\text{ES}}^{1}\\cdot T_{\\text{SC}}^{1}=T_{\\text{BE}}^{2}\\cdot T_{\\text{ES}}^{2}\\cdot T_{\\text{SC}}^{2}$$\nAs coordinate ${\\text E }$ have a fixed relation to the coordinate ${\\text S}$, we can obtian $$T_{\\text{BE}}^{2}{{\\left( T_{\\text{BE}}^{1} \\right)}^{-1}}{{T}_{\\text{BC}}}={{T}_{\\text{BC}}}{{\\left( T_{\\text{SC}}^{2} \\right)}^{-1}}T_{\\text{SC}}^{1}$$\nWe can define $\\mathbf{A}=T_{\\text{BE}}^{2}{{\\left( T_{\\text{BE}}^{1} \\right)}^{-1}}$, $\\mathbf{B}={{\\left( T_{\\text{SC}}^{2} \\right)}^{-1}}T_{\\text{SC}}^{1}$ and $\\mathbf{X}={{T}_{\\text{BC}}}$, so the probelm becomes: $$\\mathbf{AX}=\\mathbf{XB}$$\n3.2 Eye-in-hand intallment As shown in above figure, this is the eye-in-hand type installment, that is, the camera is installed in the fixed position from which the tool of robotic arm has a constant relative position. $${{T}_{\\text{BS}}}=T_{\\text{BE}}^{1}\\cdot T_{\\text{EC}}^{1}\\cdot T_{\\text{CS}}^{1}=T_{\\text{BE}}^{2}\\cdot T_{\\text{EC}}^{2}\\cdot T_{\\text{CS}}^{2}$$ As the tranformation matrix $T_{\\text{BS}}$ is constant, the following equation can be derived: $${{\\left( T_{\\text{BE}}^{2} \\right)}^{-1}}T_{\\text{BE}}^{1}{{T}_{\\text{ES}}}={{T}_{\\text{ES}}}T_{\\text{CS}}^{2}{{\\left( T_{\\text{CS}}^{1} \\right)}^{-1}}$$ where, we can define $\\mathbf{A}={{\\left( T_{\\text{BE}}^{2} \\right)}^{-1}}T_{\\text{BE}}^{1}$, $\\mathbf{B}=T_{\\text{CS}}^{2}{{\\left( T_{\\text{CS}}^{1} \\right)}^{-1}}$ and $\\mathbf{X}={{T}_{\\text{ES}}}$, so the problem become the same type: $$\\mathbf{AX}=\\mathbf{XB}$$ So the problem becomes how to solve the above equation, many previous works has been presented. Here, we give four pupolar approaches to addressing the equation like this.\n  1. A new technique for fully autonomous and efficient 3D robotics hand/eye calibration (IEEE Transactions on robotics and automation) 2. Hand-Eye Calibration (he international journal of robotics research) 3. Robot sensor calibration: solving AX= XB on the Euclidean group(IEEE Transactions on Robotics and Automation) 4. Hand-eye calibration using dual quaternions.(The International Journal of Robotics Research)   4. Alighnment In the robotic field, often find objects from the color image, and then calculate the corresponding 3D coordinate value by combining the related depth value. Before the above procedure, need to align the depth image to color image (depth registration).\nHomogeneous transformation matrix $T_{CW}$ and $T_{DW}$ can be obtained through the above calibration method, so the matrix $T_{CD}$ is $${{\\text{T}}_{\\text{CD}}}={{\\text{T}}_{\\text{CW}}}\\text{T}_{\\text{DW}}^{-1}$$ The alignned result is shown as follows. 5. Mapping depth image to point cloud In the section, we will discuss on how to reconstruct the 3 dimensional point in the real-world space from the pixel images and the corresponding depth image (alighn the depth images to the color images). Namely, after we already have the pixel point $u, v$ and depth value $\\lambda$, how we can obtain the related point cloud in thei real-world space, which is of importance in vision servoing of robotics. In the section, we know that $$\\left[ \\begin{matrix} u \\\\\nv \\\\\n1 \\\\\n\\end{matrix} \\right]=\\left[ \\begin{matrix} {{f}_{x}} \u0026amp; 0 \u0026amp; {{o}_{x}} \\\\\n0 \u0026amp; {{f}_{y}} \u0026amp; {{o}_{y}} \\\\\n0 \u0026amp; 0 \u0026amp; 1 \\\\\n\\end{matrix} \\right]\\left[ \\begin{matrix} x \\\\\ny \\\\\n1 \\\\\n\\end{matrix} \\right]$$\nwhere, $$\\begin{align} \u0026amp; x=X/Z \\nonumber\\\\\n\u0026amp; y=Y/Z \\nonumber\\\\\n\\end{align}$$\nAs $\\lambda = Z$, so $$ \\left[ \\begin{matrix} Zu \\\\\nZv \\\\\nZ \\\\\n\\end{matrix} \\right]=\\left[ \\begin{matrix} {{f}_{x}} \u0026amp; 0 \u0026amp; {{o}_{x}} \\\\\n0 \u0026amp; {{f}_{y}} \u0026amp; {{o}_{y}} \\\\\n0 \u0026amp; 0 \u0026amp; 1 \\\\\n\\end{matrix} \\right]\\left[ \\begin{matrix} X \\\\\nY \\\\\nZ \\\\\n\\end{matrix} \\right]$$\nFor the points in camera space and real-world space: $$\\left[ \\begin{matrix} X \\\\\nY \\\\\nZ \\\\\n\\end{matrix} \\right]=\\left[ \\mathbf{r},\\mathbf{t} \\right]\\left[ \\begin{matrix} U \\\\\nV \\\\\nW \\\\\n1 \\\\\n\\end{matrix} \\right]$$\nand we can make the real-world coordinate system stay the same with the camera coordinated system, so the homogeneous transformation matrix $\\left[ \\mathbf{r},\\mathbf{t} \\right]$ become unit matrix. Put all above equation together, the 3D coordinate value from the $u, v$ and $\\lambda$ $$\\begin{align} \u0026amp; U={\\lambda \\left( u-{{o}_{x}} \\right)}/{{{f}_{y}}}; \\nonumber\\\\\n\u0026amp; V={\\lambda \\left( v-{{o}_{y}} \\right)}/{{{f}_{y}}}; \\nonumber\\\\\n\u0026amp; W=\\lambda \\nonumber\\\\\n\\end{align}$$\nReferences   Comparing Three Prevalent 3D Imaging Technologies‚ÄîToF, Structured Light and Binocular Stereo Vision  A Brief Introduction to 3D Cameras  A Flexible New Technique for Camera by Zhang.   Introduction to Computer Vision CSE Department, Penn State University Instructor: Robert Collins  Camera calibration: explanning camera distortions  Automatic Radial Distortion Estimation from a Single Image  Calibration checkerboard collection  Camera calibration in Matlab  A Four-step Camera Calibration Procedure with Implicit Image Correction  Methods for Simultaneous Robot-World-Hand‚ÄìEyeCalibration: A Comparative Study  ","date":1608294609,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608294609,"objectID":"43e884ec931afd151a608be0771478f8","permalink":"/post/3dcamaratech/","publishdate":"2020-12-18T20:30:09+08:00","relpermalink":"/post/3dcamaratech/","section":"post","summary":"Recently, 3D cameras have been widely used in various computer vision applications, like robotics, autonomous driving. Leveraging the extra data provided by such sensors allows for better performance on tasks such as detection and recognition, pose estimation, 3D reconstruction, and so forth.","tags":["3D camera","Bibocular stereo vision","Time of flight","Structured light","Camera calibration","Hand-eye calibration"],"title":"Three dimensional camera techniques","type":"post"},{"authors":["Yinyin SU"],"categories":[],"content":"Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) are all efficient ways to transform the data points in high dimesion to the corresponding feature points in low dimension without losing the principal feature. Futhermore, the low-dimensional data can be visualized in frame, can feel by human. Hence, In this post, I simply summarized some mathematics model for PCA and t-SNE algorithms, implemented these methods repectively, and presented some advangtages and disadvantages of these alogorithms. At the end, some important hyperparameters that effect the performance of t-SNE are described to remind readers to choose the proper parameters. Some other demension reduction methods were summarized as follows:\n  PCA (linear); t-SNE (non-parametric/ nonlinear); Sammon mapping (nonlinear); Isomap (nonlinear); LLE (nonlinear); CCA (nonlinear); SNE (nonlinear); MVU (nonlinear); Laplacian Eigenmaps (nonlinear),   and their attributes were shown in the below figure. 1. PCA Principal Component Analysis (PCA) is a process of computing principal components using the first few principal component and ignoring the rest. The goals of PCA are to\n  extract the most important information from the data table; compress the size of the data set by keeping only this important information; simplify the description of the data set; and analyze the structure of the observations and the variables.   The PCA can be defined commonly by two methods, resulting in the same algorithme. As shown in the following figure, principal component analysis seeks a space of lower dimensionality, known as the principal subspace and denoted by the magenta line, such that the orthogonal projection of the data points (red dots) onto this subspace maximizes the variance of the projected points (green dots). An alternative definition of PCA is based on minimizing the sum-of-squares of the projection errors, indicated by the blue lines. 1.1 Maxiunum variance formulation Consider a data set of observations $\\left\\{\\mathbf{x}_{n}\\right\\}$ where $n = 1, \\cdots, N$ and the dimensionality of $\\mathbf{x}_{n}$ is $D$. The PCA can project the data on to lower dimensional space with dimensionality $M \u0026lt; D$ while maximizing the variance of the projected data. We define unit vector ${{w}_{1}}$, which is column vector, as the first principal direction and each data point $\\mathbf{x}_{n}$ is then projected onto a scalar value ${{z}_{1}^n} = \\mathbf{{w}_{1}^{T}}{{{x}}_{n}}$. Based on the above definition, we preject all data points onto $\\mathbf{w_1}$ direction, maximizing the variance along $\\mathbf{w_1}$ direction, $$\\max Var\\left( {{\\mathbf{z}}_{1}} \\right)=\\frac{1}{N}\\sum\\limits_{1}^{N}{{{\\left( {{\\mathbf{z}}_{1}}-{{{\\mathbf{\\bar{z}}}}_{1}} \\right)}^{2}}},{{\\left|| {{\\mathbf{w}}_{1}} \\right||}_{2}}=1, $$ where $\\mathbf{z}_{1}$ is $[z_1^{1}, z_2^{1}, \\cdots, z_N^{1}]$, and ${{{\\mathbf{\\bar{z}}}}_{1}}$ is sample set mean, given by ${{\\mathbf{\\bar{z}}}_{1}}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}{z_{i}^{1}}$. The above formulation is also used in other principal direction like $\\mathbf{w_2}$, so the variance of data set $\\mathbf{z_2}$ can be derived by $$\\max Var\\left( {{\\mathbf{z}}_{2}} \\right)=\\frac{1}{N}\\sum\\limits_{1}^{N}{{{\\left( {{\\mathbf{z}}_{2}}-{{{\\mathbf{\\bar{z}}}}_{2}} \\right)}^{2}}},{{\\left|| {{\\mathbf{w}}_{2}} \\right||}_{2}}=1$$ As a result, the low-dimensional data points $$\\mathbf{z}=W\\mathbf{x}, $$\nwhere $W$ is $\\left[ \\begin{matrix} {{\\mathbf{w}}_{1}} \u0026amp; {{\\mathbf{w}}_{2}} \u0026amp; {{\\mathbf{w}}_{3}} \u0026amp; \\cdots \\end{matrix} \\right]$, and it is a orthogonal matrix. The variance of data set $\\mathbf{z_1}$ can be derived\n\\begin{align} Var\\left( {{{\\mathbf{{z}}}}_{1}} \\right) \u0026amp; =\\frac{1}{N}{{\\sum\\limits_{i=1}^{N}{\\left( z_{i}^{1}-{{{\\bar{z}}}^{1}} \\right)}}^{2}} \\nonumber \\\\\n\u0026amp; =\\frac{1}{N}{{\\sum\\limits_{i=1}^{N}{\\left( \\mathbf{w}_{1}^{\\text{T}}x_{i}^{1}-\\mathbf{w}_{1}^{\\text{T}}{{{\\bar{x}}}^{1}} \\right)}}^{2}} \\nonumber\\\\\n\u0026amp; =\\frac{1}{N}{{\\sum\\limits_{i=1}^{N}{\\left[ \\mathbf{w}_{1}^{\\text{T}}\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right) \\right]}}^{2}}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}{\\mathbf{w}_{1}^{\\text{T}}\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right)\\mathbf{w}_{1}^{\\text{T}}\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right)} \\nonumber\\\\\n\u0026amp; =\\frac{1}{N}\\sum\\limits_{i=1}^{N}{\\mathbf{w}_{1}^{\\text{T}}\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right)}{{\\left[ \\mathbf{w}_{1}^{\\text{T}}\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right) \\right]}^{\\text{T}}} \\nonumber\\\\\n\u0026amp; =\\mathbf{w}_{1}^{\\text{T}}\\left[ \\frac{1}{N}\\sum\\limits_{i=1}^{N}{\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right)}{{\\left( x_{i}^{1}-{{{\\bar{x}}}^{1}} \\right)}^{\\text{T}}} \\right]{{\\mathbf{w}}_{1}} \\nonumber\\\\\n\u0026amp; = \\mathbf{w}_{1}^{\\text{T}} cov\\left( \\mathbf x\\right) \\mathbf{w}_{1}\\nonumber\\\\\n\\end{align}\nwhere $S = cov\\left( \\mathbf x\\right)$ is covariance matrix of data set $\\left\\{\\mathbf{x}_{n}\\right\\}$, and it is a sysmetric and semidefinite matrix. Eventually, if we want to obtain the principal vector $\\mathbf{w_1}$, the optimization equation is\n\\begin{align} \\underset{{{\\mathbf{w}}_{1}}}{\\mathop{\\max }},\\mathbf{w}_{1}^{\\text{T}}S{{\\mathbf{w}}_{1}},\\\\\ns.t.\\quad \\mathbf{w}_{1}^{\\text{T}}{{\\mathbf{w}}_{1}} = 1. \\end{align}\nUsing the Langrange multiplier, $$g\\left( {{\\mathbf{w}}_{1}} \\right)=\\mathbf{w}_{1}^{\\text{T}}S{{\\mathbf{w}}_{1}}+\\alpha \\left( 1-\\mathbf{w}_{1}^{\\text{T}}{{\\mathbf{w}}_{1}} \\right), $$\nlet $\\frac{\\partial g\\left( {{\\mathbf{w}}_{1}} \\right)}{\\partial {{\\mathbf{w}}_{1}}}=0$, the following equation can be obtianed $$\\frac{\\partial g\\left( {{\\mathbf{w}}_{1}} \\right)}{\\partial {{\\mathbf{w}}_{1}}}=S{{\\mathbf{w}}_{1}}-\\alpha{{\\mathbf{w}}_{1}} =0$$ Hence, the ${{\\mathbf{w}}_{1}}$ and $\\alpha$ are the eigenvector and the corresponding eigenvalue of covariance matrix $S$. Moreover, $\\alpha$ is the first largest eigenvalue. Also, the ${{\\mathbf{w}}_{2}}$ is the corresponding eigenvetor related to the second largest eigenvalue of $S$.\n2. t-SNE t-SNE is a mechine learning algorithm for visualazation developed by Sam Roweis and Geoggrey Hinton, well suitied for embedding high-dimensiontal data for visualization in a low-dimensional space of two or three dimension observed directly by human. It is extensively applied in image processing, NLP, genomic data and speech processing. The t-SNE puts emphasis on (1) modeling dissimilar datapoints by means of large pairwise distances, and (2) modeling similar datapoints by means of small pairwise distances. To keep things simple, here‚Äôs a brief overview of working of t-SNE:\n  The algorithms starts by calculating the probability of similarity of points in high-dimensional space and calculating the probability of similarity of points in the corresponding low-dimensional space. The similarity of points is calculated as the conditional probability that a point A would choose point B as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian (normal distribution) centered at A. It then tries to minimize the difference between these conditional probabilities (or similarities) in higher-dimensional and lower-dimensional space for a perfect representation of data points in lower-dimensional space. To measure the minimization of the sum of difference of conditional probability t-SNE minimizes the sum of Kullback-Leibler divergence of overall data points using a gradient descent method.   2.1 Mathematics model Step 1: Define similarity in high-dimensional space t-SNE uses Stochastic Neighbour Embedding (SNE) method to convert the high-dimensional Euclidean distances between data points into conditional probablities that represent their similiarities. The conditional probability ${{p}_{j|i}}$ is defined to represent the similarity, $x_i$ would choose $x_j$ as its neighhour if neighours are chosen in proportion to their probablity density under a Guassian with center at $x_i$. The nearer the data points are, the higher value of ${{p}_{j|i}}$ is. Mathematically, ${{p}_{j|i}}$ can be defined by $${\\displaystyle p_{j\\mid i}={\\frac {\\exp(-\\lVert \\mathbf {x} _{i}-\\mathbf {x} _{j}\\rVert ^{2}/2\\sigma _{i}^{2})}{\\sum _{k\\neq i}\\exp(-\\lVert \\mathbf {x} _{i}-\\mathbf {x} _{k}\\rVert ^{2}/2\\sigma _{i}^{2})}}}, $$ where $\\sigma_i$ is the variance of the Guassian distribution with the mean being $x_i$. Morever $p_{ij} $is defined by $${\\displaystyle p_{ij}={\\frac {p_{j\\mid i}+p_{i\\mid j}}{2N}}}$$ where $p_{ii} = 0$ and $p_{ij} = p_{ji}$.\nStep 2: Define similarity in low-dimensional space For the low-dimensional counterparts $y_i$ and $y_j$ of the high-dimensional data points $x_i$ and $x_j$. It is possible to define a similar conditional probability, donoted by $q_{ij}$ $${\\displaystyle q_{ij}={\\frac {(1+\\lVert \\mathbf {y} _{i}-\\mathbf {y} _{j}\\rVert ^{2})^{-1}}{\\sum _{k}\\sum _{l\\neq k}(1+\\lVert \\mathbf {y} _{k}-\\mathbf {y} _{l}\\rVert ^{2})^{-1}}}}, $$ where $q_{ii} = 0$.\nStep 3: Define cost function to compute $q_{ij}$ If the map points $y_i$ and $y_j$ correctly model the similarity between the high-dimensional datapoints $x_i$ and $x_j$, the conditional probabilities $p_{j|i}$ and $q_{j|i}$ will be equal. Motivated by this observation, SNE aims to find a low-dimensional data representation that minimizes the mismatch between $p_{j|i}$ and $q_{j|i}$. A natural measure of the faithfulness with which $q_{j|i}$ models $p_{j|i}$ is the Kullback-Leibler divergence (which is in this case equal to the cross-entropy up to an additive constant). SNE minimizes the sum of Kullback-Leibler divergences over all datapoints using a gradient descent method. The cost function C is given by $$C= \\sum_{}{\\displaystyle \\mathrm {KL} \\left(P\\parallel Q\\right)=\\sum _{i\\neq j}p_{ij}\\log {\\frac {p_{ij}}{q_{ij}}}}$$ in which $P$ represents the conditional probability distribution over all other datapoints given datapoint $x$, and $Q$ represents the conditional probability distribution over all other map points given map point $y$. The defination of $p_{ij}$ and $q_{ij}$ solves the crowding problem for SNE from Laurens van der Maaten in Visualizing Data using t-SNE. The gradient of the Kullback-Leibler divergence $C$ is derived (the detailed derived procedure was presented in Visualizing Data using t-SNE) $$\\frac{\\delta C}{\\delta {{y}_{i}}}=4\\sum\\limits_{j}{\\left( {{p}_{ij}}-{{q}_{ij}} \\right)}\\left( {{y}_{i}}-{{y}_{j}} \\right){{\\left( 1+{{\\left| {{y}_{i}}-{{y}_{j}} \\right|}^{2}} \\right)}^{-1}}$$\n2.2 Implementing PCA in python3 ## Inherited from personal page of Laurens van der Maaten\r## https://lvdmaaten.github.io/tsne/\rimport numpy as np\rimport matplotlib.pyplot as plt\rdef Hbeta(D=np.array([]), beta=1.0):\r\u0026quot;\u0026quot;\u0026quot;\rCompute the perplexity and the P-row for a specific value of the\rprecision of a Gaussian distribution.\r\u0026quot;\u0026quot;\u0026quot;\r# Compute P-row and corresponding perplexity\rP = np.exp(-D.copy() * beta)\rsumP = sum(P)\rH = np.log(sumP) + beta * np.sum(D * P) / sumP\rP = P / sumP\rreturn H, P\rdef x2p(X=np.array([]), tol=1e-5, perplexity=30.0):\r\u0026quot;\u0026quot;\u0026quot;\rPerforms a binary search to get P-values in such a way that each\rconditional Gaussian has the same perplexity.\r\u0026quot;\u0026quot;\u0026quot;\r# Initialize some variables\rprint(\u0026quot;Computing pairwise distances...\u0026quot;)\r(n, d) = X.shape\rsum_X = np.sum(np.square(X), 1)\rD = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\rP = np.zeros((n, n))\rbeta = np.ones((n, 1))\rlogU = np.log(perplexity)\r# Loop over all datapoints\rfor i in range(n):\r# Print progress\rif i % 500 == 0:\rprint(\u0026quot;Computing P-values for point %d of %d...\u0026quot; % (i, n))\r# Compute the Gaussian kernel and entropy for the current precision\rbetamin = -np.inf\rbetamax = np.inf\rDi = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\r(H, thisP) = Hbeta(Di, beta[i])\r# Evaluate whether the perplexity is within tolerance\rHdiff = H - logU\rtries = 0\rwhile np.abs(Hdiff) \u0026gt; tol and tries \u0026lt; 50:\r# If not, increase or decrease precision\rif Hdiff \u0026gt; 0:\rbetamin = beta[i].copy()\rif betamax == np.inf or betamax == -np.inf:\rbeta[i] = beta[i] * 2.\relse:\rbeta[i] = (beta[i] + betamax) / 2.\relse:\rbetamax = beta[i].copy()\rif betamin == np.inf or betamin == -np.inf:\rbeta[i] = beta[i] / 2.\relse:\rbeta[i] = (beta[i] + betamin) / 2.\r# Recompute the values\r(H, thisP) = Hbeta(Di, beta[i])\rHdiff = H - logU\rtries += 1\r# Set the final row of P\rP[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP\r# Return final P-matrix\rprint(\u0026quot;Mean value of sigma: %f\u0026quot; % np.mean(np.sqrt(1 / beta)))\rreturn P\rdef pca(X=np.array([]), no_dims=50):\r\u0026quot;\u0026quot;\u0026quot;\rRuns PCA on the NxD array X in order to reduce its dimensionality to\rno_dims dimensions.\r\u0026quot;\u0026quot;\u0026quot;\rprint(\u0026quot;Preprocessing the data using PCA...\u0026quot;)\r(n, d) = X.shape\rX = X - np.tile(np.mean(X, 0), (n, 1))\r(l, M) = np.linalg.eig(np.dot(X.T, X))\rY = np.dot(X, M[:, 0:no_dims])\rreturn Y\rdef tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0):\r\u0026quot;\u0026quot;\u0026quot;\rRuns t-SNE on the dataset in the NxD array X to reduce its\rdimensionality to no_dims dimensions. The syntaxis of the function is\r`Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\rNotation of the input parameters:\rno_dims: Show the dimensional datapoints in 2-dimension space.\rinitial_dims: The PCA is introduced to pre-reduce the initial dimension. After the process, Dimension become 50 from 784.\rperplexity: It is, in a sense, a guess about the number of close neighbors each point has. Its recommended value (5-50).\r\u0026quot;\u0026quot;\u0026quot;\r# Check inputs\rif isinstance(no_dims, float):\rprint(\u0026quot;Error: array X should have type float.\u0026quot;)\rreturn -1\rif round(no_dims) != no_dims:\rprint(\u0026quot;Error: number of dimensions should be an integer.\u0026quot;)\rreturn -1\r# Initialize variables\rX = pca(X, initial_dims).real\r(n, d) = X.shape\rmax_iter = 1000\rinitial_momentum = 0.5\rfinal_momentum = 0.8\reta = 500\rmin_gain = 0.01\rY = np.random.randn(n, no_dims)\rdY = np.zeros((n, no_dims))\riY = np.zeros((n, no_dims))\rgains = np.ones((n, no_dims))\r# Compute P-values\rP = x2p(X, 1e-5, perplexity)\rP = P + np.transpose(P)\rP = P / np.sum(P)\rP = P * 4.\t# early exaggeration\rP = np.maximum(P, 1e-12)\r# Run iterations\rfor iter in range(max_iter):\r# Compute pairwise affinities\rsum_Y = np.sum(np.square(Y), 1)\rnum = -2. * np.dot(Y, Y.T)\rnum = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))\rnum[range(n), range(n)] = 0.\rQ = num / np.sum(num)\rQ = np.maximum(Q, 1e-12)\r# Compute gradient\rPQ = P - Q\rfor i in range(n):\rdY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)\r# Perform the update\rif iter \u0026lt; 20:\rmomentum = initial_momentum\relse:\rmomentum = final_momentum\rgains = (gains + 0.2) * ((dY \u0026gt; 0.) != (iY \u0026gt; 0.)) + \\\r(gains * 0.8) * ((dY \u0026gt; 0.) == (iY \u0026gt; 0.))\rgains[gains \u0026lt; min_gain] = min_gain\riY = momentum * iY - eta * (gains * dY)\rY = Y + iY\rY = Y - np.tile(np.mean(Y, 0), (n, 1))\r# Compute current value of cost function\rif (iter + 1) % 10 == 0:\rC = np.sum(P * np.log(P / Q))\rprint(\u0026quot;Iteration %d: error is %f\u0026quot; % (iter + 1, C))\r# Stop lying about P-values\rif iter == 100:\rP = P / 4.\r# Return solution\rreturn Y\rif __name__ == \u0026quot;__main__\u0026quot;:\rprint(\u0026quot;Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\u0026quot;)\rprint(\u0026quot;Running example on 2,500 MNIST digits...\u0026quot;)\rX = np.loadtxt(\u0026quot;mnist2500_X.txt\u0026quot;)\rlabels = np.loadtxt(\u0026quot;mnist2500_labels.txt\u0026quot;)\rY = tsne(X, 2, 50, 20.0)\rnp.savetxt(\u0026quot;myfile.txt\u0026quot;, Y)\r# data_to_show = np.loadtxt(\u0026quot;myfile.txt\u0026quot;)\rdata_to_show = Y\rfig, ax = plt.subplots()\rscatter = plt.scatter(data_to_show[:, 0], data_to_show[:, 1], 50, labels, alpha=0.4)\rfor i, txt in enumerate(labels[0:40]):\rax.annotate(txt, (data_to_show[i, 0], data_to_show[i, 1]))\rlegend1 = ax.legend(*scatter.legend_elements(),\rloc=\u0026quot;upper left\u0026quot;, title=\u0026quot;Digit\u0026quot;)\rax.add_artist(legend1)\rax.set_title('Visualize high-dimensional data points \\n in 2-dimensional plot by t-SNE', fontsize=16)\rax.set_xlabel('Dimension 1')\rax.set_ylabel('Dimension 2')\rplt.savefig('t-SNE.png')\rplt.show()\r As can be seen in the following figure, the visualized high-dimensional mnist dataset is shown in the lower 2-dimensional space. 2.3 How to give the hyper parameters   n_components: Dimension of the embedded space, this is the lower dimension that we want the high dimension data to be converted to. The default value is 2 for 2-dimensional space. Perplexity: The perplexity is related to the number of nearest neighbors that are used in t-SNE algorithms. Larger datasets usually require a larger perplexity. Perplexity can have a value between 5 and 50. The default value is 30. n_iter: Maximum number of iterations for optimization. Should be at least 250 and the default value is 1000 learning_rate: The learning rate for t-SNE is usually in the range [10.0, 1000.0] with the default value of 200.0.   3. Comparation between PCA and t-SNE   The PCA is a linear algorithm which is not able to interpret complex ploynomial relationship between features. By contrast, t-SNE is actived based on probability distributions on neighbourhood graph to understand the structure with the data; The linear dimension reduction algorithm (PCA) concentrates on placing dissimilar data points far apart in a lower dismension representation, while the nonlinear algorithm (t-SNE) places the similar datapoints closely together in lower dimesional space. Hence, as can be seen in the following figure, PCA can only capture linear structures in the features. The t-SNE algorithm works in a very different way and focuses to preserve the local distances of the high-dimensional data in some mapping to low-dimensional data.    import numpy as np\rimport matplotlib.pyplot as plt\rfrom sklearn.decomposition import PCA\rfrom sklearn.manifold import TSNE\rX = np.loadtxt(\u0026quot;mnist2500_X.txt\u0026quot;)\rlabels = np.loadtxt(\u0026quot;mnist2500_labels.txt\u0026quot;)\r## implementing by PCA\rpca = PCA(n_components=2, svd_solver='full')\rdata_to_show_PCA = pca.fit_transform(X)\r## implementing by t-SNE\rdata_to_show_tSNE = TSNE(n_components=2).fit_transform(X)\r## Figure PCA\rfig, ax = plt.subplots(1,2,figsize=(20, 8))\rscatter1 = ax[0].scatter(data_to_show_PCA[:, 0], data_to_show_PCA[:, 1], 50, labels, alpha=0.4)\rfor i, txt in enumerate(labels[0:50]):\rax[0].annotate(txt, (data_to_show_PCA[i, 0], data_to_show_PCA[i, 1]))\rax[0].set_title('PCA', fontsize=16)\rax[0].set_xlabel('Principal 1')\rax[0].set_ylabel('Principal 2')\rlegend1 = ax[0].legend(*scatter1.legend_elements(),\rloc=\u0026quot;upper left\u0026quot;, title=\u0026quot;Digit\u0026quot;)\rax[0].add_artist(legend1)\rfig.suptitle('Visualize high-dimensional data points through \\n PCA and t-SNE', fontsize=16)\r## Figure t-SNE\rscatter2 = ax[1].scatter(data_to_show_tSNE[:, 0], data_to_show_tSNE[:, 1], 50, labels, alpha=0.4)\rfor i, txt in enumerate(labels[0:50]):\rax[1].annotate(txt, (data_to_show_tSNE[i, 0], data_to_show_tSNE[i, 1]))\rax[1].set_title('t-SNE', fontsize=16)\rax[1].set_xlabel('Dimension 1')\rax[1].set_ylabel('Dimension 2')\rlegend2 = ax[1].legend(*scatter2.legend_elements(),\rloc=\u0026quot;upper left\u0026quot;, title=\u0026quot;Digit\u0026quot;)\rax[1].add_artist(legend2)\rplt.savefig('PCA_t-SNE.png')\rplt.show()\r References   A tutorial on Principal Components Analysis  Principal component analysis  PCA: A Practical Guide to Principal Component Analysis in R \u0026amp; Python  A simple introduction to PCA.  Chaper12: Continous latent variables in Pattern recognition and machine learning.  t-SNE Walkthrough  Good hyperparameter Information  Laurens van der Maaten personal page  Laurens van der Maaten\u0026rsquo; talk about t-SNE  What advantages does the t-SNE algorithm have over PCA?  ","date":1603369809,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603369809,"objectID":"d5d02082d29eb859326c553470b0d1be","permalink":"/post/t-sne/","publishdate":"2020-10-22T20:30:09+08:00","relpermalink":"/post/t-sne/","section":"post","summary":"Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) are all efficient ways to transform the data points in high dimesion to the corresponding feature points in low dimension without losing the principal feature.","tags":["PCA","t-SNE","Machine learning"],"title":"Dimension Reduction and High Dimensional Data Visualization by PCA and t-SNE","type":"post"},{"authors":["Yinyin SU"],"categories":[],"content":"In machine learning, the perceptron is an algorithm for supervised learning of binary classfiers. The perceptron algorithm was invented 1958 at the Cornell Aeronautical lab by Frank Rosenblatt.\nPerceptron model For input space (featured space) $\\mathcal{X} = {{\\mathbf{R}}^{n}}$ and output space $\\mathcal{Y} =\\{-1, +1\\}$, the perceptron model can be built by: $$f(x) = sign(\\omega \\centerdot x+b)$$ $$sign(x) = \\begin{cases} +1 \u0026amp; \\text{if } x \\ge 0,\\\\\n-1 \u0026amp; \\text{if } x \u0026lt; 0. \\end{cases}$$ where $\\omega$ and $b$ are the weight and bias for the above function respectively. Through being trained by the give training data $T = \\{ (x_0, y_0), (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\}$, where $x_i \\in \\mathcal{X} = {{\\mathbf{R}}^{n}}$ and $y_i \\in \\mathcal{Y}=\\{-1, +1\\}$, $i = 1, 2, \\cdots, N$. The parameters $\\omega$ and $b$ can be leanned through the mentioned data set. The learned linear hyperplane can divide the training data into two parts labelling +1 and -1. The schematic diagram is shown as follows.\n The schematic diagram of perceptron   Perceptron strategy In perceptron algorithm, the total distance to the hyperplane is defined to depict its cost function: $$L\\left( \\omega ,b \\right)=-\\sum\\limits_{{{x}_{i}}\\in M}{{{y}_{i}}\\left( \\omega \\cdot {{x}_{i}}+b \\right)}$$ where $M$ is misclassfication data set.\nPerceptron learning algorithm For the given training data set, the parameters $\\omega$ and $b$ can be optimized by minimizing the cost function: $$\\underset{\\omega ,b}{\\mathop{\\min }},L\\left( \\omega ,b \\right)=-\\sum\\limits_{{{x}_{i}}\\in M}{{{y}_{i}}\\left( \\omega \\cdot {{x}_{i}}+b \\right)}$$ We can use stochastic gradient descent method to solve the above optimized problem. Take partial derivative with respect to $\\omega$ and $b$, the iteration strategy can be obtained: $$\\begin{align} \u0026amp; \\omega \\leftarrow \\omega +\\eta {{y}_{i}}{{x}_{i}} \\\n\u0026amp; b\\leftarrow b+\\eta {{y}_{i}} \\\n\\end{align}$$ where $\\eta$ is a learning rate hyperparameter set by user.\nBased on the aforementioned discussion, the perceptron algorithm can be derived: $\\omega$, $b$\n  Preceptron algorithm   Input: training data set: $T = { (x_0, y_0), (x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)}$, $i = 1, 2, 3, \\cdots, N$; learning rate $\\eta (0 \u0026lt; \\eta \\le 1)$; Output: $\\omega$, $b$ and preceptron model: $f(x) = sign(\\omega \\centerdot x+b)$ Initialize $\\omega_0, b_0$; while: Choosing the single instance $(x_i, y_i)$ from $T$ until no misclassfication data contained in the set; ‚ÄÉif $y_i \\left( \\omega \\cdot x_i +b\\right) \\le 0$ ‚ÄÉ$\\omega \\leftarrow \\omega +\\eta {y_i}{x_i}$ and $b\\leftarrow b+\\eta {{y}_{i}}$ ‚ÄÉendif end while   Implementing the algorithm in python3 from matplotlib import pyplot as plt import numpy as np from matplotlib.animation import FuncAnimation # define a function to realize preceptron algorithm. def PrecessingPreceptron(point, learning_rate): w_set = [] b_set = [] w = np.array([0,0]) b = 0 flag = 1 while flag: flag = 0 i = 0 for i in range(len(point)): if point[i,2]*(np.matmul(w, point[i,0:2]) + b) \u0026lt;= 0: w = w + learning_rate * point[i,0:2] * point[i,2] b = b + learning_rate * point[i,2] w_set.append(w) b_set.append(b) flag = 1 break if flag == 0: break return w_set, b_set # DrawFigure is used to draw the process def DrawFigure(count, w_set, b_set, gap): point_x_1 = 0 point_x_2 = 5 if count == 0: line.set_data([], []) return line, if gap*count \u0026lt; len(w_set): w = w_set[count*gap] b = b_set[count*gap] else: w = w_set[-1] b = b_set[-1] if w[1] != 0: point_y_1 = -(w[0] * point_x_1 + b) / w[1] point_y_2 = -(w[0] * point_x_2 + b) / w[1] thisx = np.array([point_x_1, point_x_2]) thisy = np.array([point_y_1, point_y_2]) line.set_data(thisx, thisy) return line, if __name__ == \u0026quot;__main__\u0026quot;: # define input data set traning_data = np.array([[3, 4, 1],[4, 3, 1], [3,3,-1],[1,2,-1],[3,5,1],[4,7,1],[3,2,-1],[1.5,5,-1]]) learning_rate = 0.7 fig, ax = plt.subplots() line, = ax.plot([], [], 'k-') for index in range(len(traning_data)): if traning_data[index,2] == 1: ax.plot(traning_data[index, 0], traning_data[index, 1], 'ro') else: ax.plot(traning_data[index,0], traning_data[index,1],'bo') plt.xlabel('$x_1$') plt.ylabel('$x_2$') plt.title('Preceptron algorithm learning precess') w_set, b_set = PrecessingPreceptron(traning_data, learning_rate) anim = FuncAnimation(fig, DrawFigure,frames=np.arange(0, 15), fargs=(w_set, b_set, 50), interval=300, blit=True) anim.save('preceptron.gif', writer='imagemagick') plt.show()  Results: Notes   It can be testified that the perceptron algorithm could converge at last within the limited steps of iterations. The original algorithm have multiple solutions coming from: 1. The initial parameters $\\omega_0, b_0$; 2. the selection order of misclassficated point in the learning process.   ","date":1599136209,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599136209,"objectID":"80f24f9802944cd6d9522547225bd036","permalink":"/post/preceptron/","publishdate":"2020-09-03T20:30:09+08:00","relpermalink":"/post/preceptron/","section":"post","summary":"In machine learning, the perceptron is an algorithm for supervised learning of binary classfiers. The perceptron algorithm was invented 1958 at the Cornell Aeronautical lab by Frank Rosenblatt.\nPerceptron model For input space (featured space) $\\mathcal{X} = {{\\mathbf{R}}^{n}}$ and output space $\\mathcal{Y} =\\{-1, +1\\}$, the perceptron model can be built by: $$f(x) = sign(\\omega \\centerdot x+b)$$ $$sign(x) = \\begin{cases} +1 \u0026amp; \\text{if } x \\ge 0,\\\\","tags":["Preceptron","Machine learning"],"title":"Preceptron algorithm","type":"post"},{"authors":["Yinyin SU"],"categories":[],"content":"ROS is an open-source, meta-operating system for your robot. Some advantages are as follows.\n Distributed computation Software resure Rapid testing  Packages rospack list # You can obtain a list of all of the installed ROS packages. rospack find package-name # find the directory of a single package rosls package-name # view all files in the package directory roscd package-name # go to the package directory directly  Nodes ROS node communication network  ROS node communication network   rosrun package-name executable-name # Starting a node rosnode list #listing nodes rosnode info node-name # get some information abour a particular node. rosnode kill node-name # Killing a node. rosnode cleanup #remove the dead nodes from the list. rqt_graph # veiwing the graph rosmsg show message-type-name #inspecting a message type. rostopic pub -r rate-in-hz topic-name message-type message-content # publishing messages from the command line. rosrun turtlesim turtlesim_node __name:=A rosrun turtlesim turtlesim_node __name:=B # run the same node with different name, as ROS master does not allow multiple nodes with the same name. roswtf # perform a broad variety of sanity checks.  Creating a workplace and a package  The file structure of workplace   user@hostname$ mkdir -p ~/catkin_ws/src user@hostname$ cd ~/catkin_ws/src user@hostname$ catkin_init_workspace user@hostname$ source devel/setup.bash user@hostname$ cd ~/catkin_ws/src user@hostname$ catkin_create_pkg my_package_name # creating two default versions of these two configuration files: package.xml and CMakeLists.txt.  The first program \u0026lsquo;hello world\u0026rsquo; in ROS  Step 1: Write the helle world program  // This is a ROS version fo the standard \u0026quot;hello world\u0026quot; program // This header defines the standard ROS classes. #include \u0026lt;ros/ros.h\u0026gt; int main(int agrc, char** agrv){ //Initialize the ROS system. ros::init(argc, argv, \u0026quot;hello_ros\u0026quot;); // Establish this program as a ROS node ros::NodeHandle nh; //Send some output as a log message. ROS_INFO_STREAM(\u0026quot;Hello, ROS!\u0026quot;); return 0; }   Step 2: Compiling the hello world program    Declaring dependencies   CMakeLists.txt find_package(catkin REQUIRED COMPONENTS package-names) package.xml \u0026lt;build_depend\u0026gt;package-name\u0026lt;/build_depend\u0026gt; \u0026lt;run_depend\u0026gt;package-name\u0026lt;/run_depend\u0026gt;   Declaring an executable   CMakeLists.txt add_executable(executable-name source-files1 source-files2 ...) target_link_libraries(executable-name ${catkin_LIBRARIES})   Building the workplace catkin_make source devel/setup.bash   Write a publisher node and a subscriber node  Be mindful of the lifetime of your ros::Publisher objects. Creating the publisher is an expensive operation, so it\u0026rsquo;s a usually a bad idea to creat a new ros::Publisher object each time you want to publish a message. Refer the ROS wiki Publisher.  // This program publishes randomly-generated velocity messages for turtlesim. #include\u0026lt;ros/ros.h\u0026gt; // incldue message type declaration #include\u0026lt;geometry_msgs/Twist.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; int main(int agrc, char** agrv){ //Initialize the ROS system and become a node ros::init(agrc, agrv, \u0026quot;publish_velocity\u0026quot;); ros::NodeHandle nh; //creat a publisher object // ros::Publisher pub = node_handle.advertise\u0026lt;message_type\u0026gt;(topic_name, queue_size); // queue size is an integer representing the size of the message queue for publisher. In most case, a reasonablly large value, say 1000, is suitable. ros::Publisher pub = nh.advertise\u0026lt;geometry_msgs::Twist\u0026gt;(\u0026quot;turtle1/cmd_vel\u0026quot;, 1000); //send the random number generator. srand(time(0)); //loop at 2 Hz until the node is shut down. ros::Rate rate(2); // ros::ok() function is used to check for node shutdown. while(ros::ok()){ // creat and fill in the message. The other four fields, which are ignored by turtlesim, default to 0. geometry_msgs::Twist msg; msg.linear.x = double(rand())/double(RAND_MAX); msg.linear.z = 2*double(rand())/double(RAND_MAX) - 1; //Publish the message. pub.publish(msg); //send a message to rosout with the details. ROS_INFO_STREAM(\u0026quot;Sending random velocity command: \u0026quot;\u0026lt;\u0026lt; \u0026quot; linear=\u0026quot; \u0026lt;\u0026lt; msg.linear.x \u0026lt;\u0026lt; \u0026quot; angular=\u0026quot; \u0026lt;\u0026lt; msg.angular.z); //Wait until it's time for another iteration. rate.sleep(); } return 0; }   Refer to ROS wiki Subscriber  //This program subscribes to turtle 1 /pose and show its message on the sreen. #include\u0026lt;ros/ros.h\u0026gt; #include\u0026lt;turtlesim/ Pose.h\u0026gt; #include\u0026lt;iomanip\u0026gt; // for std::setprecision and std::fixed //A callback function. Excuted each time a new pose // message arrives. void poseMessageReceived(const turtlesim::Pose\u0026amp; msg){ ROS_INFO_STREAM(std::setprecision(2) \u0026lt;\u0026lt; std::fixed \u0026lt;\u0026lt; \u0026quot;position =(\u0026quot; \u0026lt;\u0026lt; msg.x \u0026lt;\u0026lt; \u0026quot;, \u0026quot; \u0026lt;\u0026lt; msg.y \u0026lt;\u0026lt; \u0026quot;)\u0026quot; \u0026lt;\u0026lt; \u0026quot; direction=\u0026quot; \u0026lt;\u0026lt; msg.theta); int main(int agrc, char** agrv){ // Initialize the ROS system and become a node. ros::init(agrc, agrv, \u0026quot;subscribe_to_pose\u0026quot;); ros::NodeHandle nh; //creat a subcriber object // ros::Subscriber sub = node_handle.subscribe(topic_name, queue_size, pointer_to_callback_function); //Most of parameters have analogues in the declaration of a ros::Publisher object. ros::Subscriber sub = nh.subscribe(\u0026quot;turtle1/pose\u0026quot;. 1000, \u0026amp;poseMessageReceived); //let ROS take over. ros::spin(); } }    The relationship between ros::spin() and ros::spinonce()    ROS will only execute our callback function when we give it explicit permission to do so by using ros::spin() or ros::spinonce() ros::spin() will be called all the time, while ros::spinonce() is called only one time. Hence, the main process will block when you use ros::spin() function until the the node shutdown. ros::spin() equals to  while(ros::ok()){ ros::spinonce(); }   If you want to subscribe and publish message using the same node, you can refer to this example by Dhruv Ilesh Shah.   ","date":1597131261,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597131261,"objectID":"76b0827d471e537d55e48ae496e0d489","permalink":"/post/someusefulcommandinros/","publishdate":"2020-08-11T15:34:21+08:00","relpermalink":"/post/someusefulcommandinros/","section":"post","summary":"ROS is an open-source, meta-operating system for your robot. Some advantages are as follows.\n Distributed computation Software resure Rapid testing  Packages rospack list # You can obtain a list of all of the installed ROS packages.","tags":["ROS","Ubuntu"],"title":"Some useful commands for ROS in Ubuntu","type":"post"},{"authors":["Yinyin SU","Zhonggui Fang","Wenpei Zhu","Xiaochen Sun","Yuming Zhu","Hexiang Wang","Hailin Huang","Sicong Liu","Zheng Wang"],"categories":null,"content":" The file structure of workplace   --  Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   -- ","date":1581465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581465600,"objectID":"29cddef5cb35112173080c6ccebea2f5","permalink":"/publication/journal-article-1/","publishdate":"2020-02-12T00:00:00Z","relpermalink":"/publication/journal-article-1/","section":"publication","summary":"Hybrid robotic gripper, soft origamic actuators, POSA joint, higher force capability, proprioception.","tags":["proprioception","soft origamic actuators"],"title":"A High-Payload Proprioceptive Hybrid Robotic Gripper With Soft Origamic Actuators","type":"publication"},{"authors":["Qi Kang","Jia Wang","Li Duan","Yinyin SU","Jianwu He","Di Wu","Wenrui Hu"],"categories":null,"content":" The file structure of workplace   --  Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   -- ","date":1555459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"d1f0f41091b62eaace3490eb556fa330","permalink":"/publication/journal-article-3/","publishdate":"2019-04-17T00:00:00Z","relpermalink":"/publication/journal-article-3/","section":"publication","summary":"Marangoni convection, pattern formation, parametric instability.","tags":["Microgravity fluid","Shijian-10"],"title":"The volume ratio effect on flow patterns and transition processes of thermocapillary convection","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"Display some concact information about me!","tags":null,"title":"Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"/experience/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"Hello!","tags":null,"title":"Experiences","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"Display some concact information about me!","tags":null,"title":"Projects","type":"widget_page"},{"authors":["Yinyin SU","Yuquan Wang","Abderrahmane Kheddar"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"d3b976677055f6238fa12f54d25dc9ef","permalink":"/publication/conference-paper-1/","publishdate":"2019-01-24T00:00:00Z","relpermalink":"/publication/conference-paper-1/","section":"publication","summary":"Task analysis, Optimization,Bayes methods, Robots, Tuning, Linear programming.","tags":["Bayesian optimization","Gaussian process"],"title":"Sample-Efficient Learning of Soft Task Priorities Through Bayesian Optimization","type":"publication"},{"authors":["Yinyin SU","Di Wu","Li Duan","Qi Kang","Peishi Lyu","Sheng Xu","Chunfeng Lao","Huacheng Song","Jingjing Zhang"],"categories":null,"content":" The file structure of workplace   --  Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   -- ","date":1514851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514851200,"objectID":"0178ebac3243c0da1a0e147a0926fb90","permalink":"/publication/journal-article-2/","publishdate":"2018-01-02T00:00:00Z","relpermalink":"/publication/journal-article-2/","section":"publication","summary":"micro-gravity environment; centrifugal cone-shaped two-phase washing machine; VOF method; micro-gravity fluid management; numerical simulation.","tags":["Microgravity washing machine"],"title":"Numerical Simulation of Flow Field in Centrifugal Cone-shaped Two-phase Washing Machine under Microgravity","type":"publication"},{"authors":["Yongqiang Li","Mingzhu Hu","Ling Liu","Yinyin SU","Li Duan","Qi Kang"],"categories":null,"content":" The file structure of workplace   --  Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   -- ","date":1434585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1434585600,"objectID":"bf92a6b924ce657024ce9f5f097404f9","permalink":"/publication/journal-article-4/","publishdate":"2015-06-18T00:00:00Z","relpermalink":"/publication/journal-article-4/","section":"publication","summary":"Rounded wall, Capillary driven flow, Approximate analytical solution, Liquid‚Äôs front position.","tags":["Microgravity fluid"],"title":"Study of Capillary Driven Flow in an Interior Corner of Rounded Wall under Microgravity","type":"publication"}]